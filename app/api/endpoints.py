from fastapi import APIRouter, File, UploadFile, Form, HTTPException, Request, BackgroundTasks
from fastapi.responses import JSONResponse, StreamingResponse
import shutil
import os
import time
import uuid # For unique document IDs
import requests
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path

# ì„œë¹„ìŠ¤ ëª¨ë“ˆ ì„í¬íŠ¸
from app.services.ocr_service import extract_multimodal_content_from_pdf, extract_text_from_pdf
from app.services.text_processing_service import split_text_into_chunks, get_embeddings
from app.services.vector_db_service import get_all_documents, delete_document, delete_all_documents, get_document_info
from app.services.vector_db_service import store_multimodal_content, search_multimodal_content, delete_multimodal_document, get_multimodal_document_info, delete_all_multimodal_documents
from app.services.multimodal_llm_service import process_multimodal_llm_chat_request, enhance_response_with_media_references
from app.services.streaming_service import process_multimodal_llm_chat_request_stream
from app.config import settings
from app.utils.logging_config import get_logger
from app.utils.security import FileValidator, sanitize_input, validate_document_id
from app.utils.monitoring import get_monitor
from app.utils.file_manager import DocumentFileManager
from app.utils.query_validator import QueryValidator
from app.services.fallback_response_service import FallbackResponseService
from app.utils.exceptions import ValidationError, FileProcessingError, OCRError, VectorDBError, EmbeddingError, LLMError

logger = get_logger(__name__)

# Pydantic ëª¨ë¸ (ìš”ì²­/ì‘ë‹µ ë³¸ë¬¸ ì •ì˜)
from pydantic import BaseModel
from typing import List, Optional, Dict, Any

class ChatRequest(BaseModel):
    query: str
    document_id: Optional[str] = None # (í•˜ìœ„ í˜¸í™˜)
    document_ids: Optional[List[str]] = None # ì—¬ëŸ¬ ë¬¸ì„œ IDì— ëŒ€í•´ ì§ˆë¬¸í•  ê²½ìš°
    model_name: Optional[str] = settings.OLLAMA_DEFAULT_MODEL
    lang: Optional[str] = "ko"
    conversation_history: Optional[List[Dict[str, str]]] = None # ëŒ€í™” íˆìŠ¤í† ë¦¬

class ChatResponse(BaseModel):
    query: str
    response: str
    source_document_id: Optional[str] = None
    retrieved_chunks_preview: Optional[List[str]] = None # ë””ë²„ê¹…/ì •ë³´ìš©
    content_summary: Optional[Dict[str, int]] = None # ë©€í‹°ëª¨ë‹¬ ì»¨í…ì¸  ìš”ì•½
    media_references: Optional[Dict[str, Any]] = None # ì´ë¯¸ì§€/í‘œ ì°¸ì¡° ì •ë³´

router = APIRouter()

# ì—…ë¡œë“œ ë””ë ‰í† ë¦¬ëŠ” settingsì—ì„œ ê´€ë¦¬

# PDF ì²˜ë¦¬ ìƒíƒœ ì €ì¥ (ê°„ë‹¨í•œ ì¸ë©”ëª¨ë¦¬ ë°©ì‹)
pdf_processing_status = {}

executor = ThreadPoolExecutor(max_workers=settings.MAX_CONCURRENT_FILE_PROCESSING)  # ìµœëŒ€ ë™ì‹œ íŒŒì¼ ì²˜ë¦¬ ê°œìˆ˜

from app.services.text_processing_service import process_multimodal_pdf_and_store

def process_pdf_background_entry(
    file_path,
    document_id,
    filename: str,
    ocr_correction_enabled: bool,
    llm_correction_enabled: bool
):
    # ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ìš© ì—”íŠ¸ë¦¬ í•¨ìˆ˜ (threaded)
    from app.api.endpoints import process_pdf_background
    process_pdf_background(
        file_path,
        document_id,
        filename,
        ocr_correction_enabled,
        llm_correction_enabled
    )

def process_pdf_background(
    file_path: str,
    document_id: str,
    filename: str,
    ocr_correction_enabled: bool,
    llm_correction_enabled: bool
):
    """
    ë°±ê·¸ë¼ìš´ë“œì—ì„œ PDFì˜ ëª¨ë“  ì½˜í…ì¸ (í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, í‘œ)ë¥¼ ì¶”ì¶œí•˜ê³  ì €ì¥í•©ë‹ˆë‹¤.
    """
    logger.info(f"ğŸš€ Background task started: Processing PDF {document_id} from {file_path}")
    logger.info(f"Correction settings: OCR={ocr_correction_enabled}, LLM={llm_correction_enabled}")
    
    # ê¸°ì¡´ ìƒíƒœ ì •ë³´ ìœ ì§€í•˜ë©´ì„œ ì²˜ë¦¬ ì‹œì‘ ìƒíƒœë¡œ ì—…ë°ì´íŠ¸
    current_status = pdf_processing_status.get(document_id, {})
    was_queued = current_status.get("step") == "Queued"
    
    # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹œì‘ í™•ì¸ ìƒíƒœë¡œ ì—…ë°ì´íŠ¸
    pdf_processing_status[document_id] = {
        "step": "Starting",
        "message": "íì—ì„œ ì²˜ë¦¬ ì‹œì‘ë¨" if was_queued else "ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ ì‹œì‘ë¨",
        "percent": 1,
        "current_page": 0,
        "total_pages": 0,
        "details": {"started": True, "was_queued": was_queued},
        "timestamp": __import__('datetime').datetime.now().isoformat()
    }
    logger.info(f"âœ… Background processing status updated for: {document_id}")
    
    def update_status(step: str, message: str, percent: int, current_page: int = 0, total_pages: int = 0, 
                     details: dict = None):
        """ìƒì„¸í•œ ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸"""
        status = {
            "step": step,
            "message": message,
            "percent": percent,
            "current_page": current_page,
            "total_pages": total_pages,
            "details": details or {},
            "timestamp": __import__('datetime').datetime.now().isoformat()
        }
        pdf_processing_status[document_id] = status
        logger.info(f"[Task {document_id}] {step}: {message} ({percent}%)")
    
    try:
        # 0. PDF ë¶„ì„ ì‹œì‘
        update_status("Analyzing", "PDF íŒŒì¼ ë¶„ì„ ì¤‘...", 5)
        
        # PDF í˜ì´ì§€ ìˆ˜ í™•ì¸
        import fitz  # PyMuPDF
        try:
            doc = fitz.open(file_path)
            total_pages = len(doc)
            doc.close()
            logger.info(f"[Task {document_id}] PDF has {total_pages} pages")
        except Exception as e:
            logger.warning(f"[Task {document_id}] Could not determine page count: {e}")
            total_pages = 0
        
        # 1. ë©€í‹°ëª¨ë‹¬ ì½˜í…ì¸  ì¶”ì¶œ
        update_status("OCR", f"OCR ë° ì½˜í…ì¸  ì¶”ì¶œ ì‹œì‘... (ì´ {total_pages}í˜ì´ì§€)", 10, 0, total_pages)
        
        logger.info(f"[Task {document_id}] Step 1: Extracting multimodal content...")
        try:
            # OCR ì§„í–‰ë¥  ì½œë°± í•¨ìˆ˜ (ìƒì„¸ ë‹¨ê³„ í¬í•¨)
            def ocr_progress_callback(current_page: int, total_pages: int, stage: str, custom_message: str = None):
                page_progress = (current_page / total_pages) if total_pages > 0 else 0
                overall_progress = 10 + (page_progress * 40)  # ë¹ ë¥¸ ì¶”ì¶œì€ 10-50% ì°¨ì§€
                
                stage_messages = {
                    "text": f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘... ({current_page}/{total_pages} í˜ì´ì§€)",
                    "images": f"ì´ë¯¸ì§€ ì¶”ì¶œ ì¤‘... ({current_page}/{total_pages} í˜ì´ì§€)",  
                    "tables": f"í‘œ ì¶”ì¶œ ì¤‘... ({current_page}/{total_pages} í˜ì´ì§€)",
                    "table_preprocessing": f"í‘œ ì „ì²˜ë¦¬ ì¤‘... ({current_page}/{total_pages} í˜ì´ì§€)",
                    "table_detection": f"í‘œ êµ¬ì¡° ë¶„ì„ ì¤‘... ({current_page}/{total_pages} í˜ì´ì§€)",
                    "table_processing": custom_message or f"í‘œ ì²˜ë¦¬ ì¤‘... ({current_page}/{total_pages} í˜ì´ì§€)",
                    "table_ocr": custom_message or f"í‘œ OCR ì¤‘... ({current_page}/{total_pages} í˜ì´ì§€)"
                }
                
                message = custom_message or stage_messages.get(stage, f"í˜ì´ì§€ ì²˜ë¦¬ ì¤‘... ({current_page}/{total_pages})")
                details = {
                    "stage": stage,
                    "pages_processed": current_page,
                    "images_found": 0,
                    "tables_found": 0,
                    "substage": stage if stage.startswith("table_") else None
                }
                
                update_status("OCR", message, int(overall_progress), current_page, total_pages, details)
            
            content_data = extract_multimodal_content_from_pdf(
                file_path,
                document_id,
                ocr_correction_enabled,
                llm_correction_enabled,
                progress_callback=ocr_progress_callback
            )
            extracted_text = content_data.get('text', '')
            extracted_images = content_data.get('images', [])
            extracted_tables = content_data.get('tables', [])
            
            if not extracted_text and not extracted_images and not extracted_tables:
                raise OCRError("No content extracted from PDF", "EMPTY_EXTRACTION")
            
            # ë¹ ë¥¸ ì¶”ì¶œ ì™„ë£Œ ìƒíƒœ
            extract_details = {
                "text_length": len(extracted_text),
                "images_count": len(extracted_images),
                "tables_count": len(extracted_tables),
                "pages_processed": total_pages
            }
            update_status("FastExtract", f"ë¹ ë¥¸ ì¶”ì¶œ ì™„ë£Œ! í…ìŠ¤íŠ¸: {len(extracted_text)}ì, ì´ë¯¸ì§€: {len(extracted_images)}ê°œ, í‘œ: {len(extracted_tables)}ê°œ", 
                         50, total_pages, total_pages, extract_details)
            
            logger.info(f"[Task {document_id}] Extracted: text={len(extracted_text)} chars, images={len(extracted_images)}, tables={len(extracted_tables)}")

        except (OCRError, FileProcessingError) as e:
            update_status("Error", f"OCR ì˜¤ë¥˜: {e.message}", 0, 0, total_pages, {"error": str(e)})
            logger.error(f"[Task {document_id}] OCR error: {e}")
            # ì‹¤íŒ¨ ì‹œ íŒŒì¼ ì •ë¦¬
            _cleanup_failed_processing(file_path, document_id)
            return

        # 2-4. ì„¸ë¶„í™”ëœ í…ìŠ¤íŠ¸ ì²˜ë¦¬ (ì²­í‚¹, ì„ë² ë”©, ì €ì¥)
        logger.info(f"[Task {document_id}] Step 2-4: Processing text content...")
        
        try:
            # ì„¸ë°€í•œ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì§ì ‘ ì‹¤í–‰ (ì„¸ë¶„í™”ëœ ì§„í–‰ë¥  í¬í•¨)
            from app.services.text_processing_service import split_text_into_chunks_with_progress, get_embeddings
            from app.services.vector_db_service import store_multimodal_content
            
            # í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì„¸ë°€í•œ ì²­í‚¹ ì²˜ë¦¬
            if extracted_text and extracted_text.strip():
                # ì„¸ë°€í•œ í…ìŠ¤íŠ¸ ë¶„í•  ë‹¨ê³„ë³„ ì½œë°±
                def detailed_chunking_callback(total_pages, current_page, step, message):
                    step_percentages = {
                        "text_preprocessing": 56,
                        "text_splitting": 58, 
                        "chunk_validation": 60,
                        "chunk_correction": 61,
                        "chunk_preparation": 62
                    }
                    percent = step_percentages.get(step, 60)
                    details = {
                        "stage": step,
                        "text_length": len(extracted_text),
                        "processed": True
                    }
                    update_status(step, message, percent, total_pages, total_pages, details)
                
                # ì„¸ë°€í•œ ì²­í¬ ë¶„í•  ì‹¤í–‰
                text_chunks = split_text_into_chunks_with_progress(
                    extracted_text,
                    progress_callback=detailed_chunking_callback,
                    total_pages=total_pages
                )
                
                # ì„ë² ë”© ìƒì„± (65%)
                update_status("Embedding", f"{len(text_chunks)}ê°œ ì²­í¬ ì„ë² ë”© ìƒì„± ì¤‘...", 65, total_pages, total_pages, 
                            {"chunks_count": len(text_chunks)})
                text_embeddings = get_embeddings(text_chunks)
                
                # ë©”íƒ€ë°ì´í„° ì¤€ë¹„ (75%)
                update_status("Metadata", "ë©”íƒ€ë°ì´í„° ì¤€ë¹„ ì¤‘...", 75, total_pages, total_pages, {})
                text_metadatas = [
                    {"source_document_id": document_id, "filename": filename, "chunk_index": i, "content_type": "text"}
                    for i in range(len(text_chunks))
                ]
                
                # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ (80%)
                update_status("Storing", "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ ì¤‘...", 80, total_pages, total_pages, {})
                store_multimodal_content(
                    document_id=document_id,
                    content_data={
                        "text_chunks": text_chunks,
                        "images": extracted_images,
                        "tables": extracted_tables
                    },
                    text_vectors=text_embeddings,
                    text_metadatas=text_metadatas
                )
                
                result = {
                    "text_chunks_stored": len(text_chunks),
                    "images_stored": len(extracted_images),
                    "tables_stored": len(extracted_tables)
                }
            else:
                # í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ì´ë¯¸ì§€/í‘œë§Œ ì €ì¥
                update_status("Storing", "ì´ë¯¸ì§€/í‘œ ë°ì´í„° ì €ì¥ ì¤‘...", 80, total_pages, total_pages, {})
                store_multimodal_content(
                    document_id=document_id,
                    content_data={
                        "text_chunks": [],
                        "images": extracted_images,
                        "tables": extracted_tables
                    },
                    text_vectors=[],
                    text_metadatas=[]
                )
                
                result = {
                    "text_chunks_stored": 0,
                    "images_stored": len(extracted_images),
                    "tables_stored": len(extracted_tables)
                }
            
            text_chunks = result.get("text_chunks_stored", 0)
            extracted_images = result.get("images_stored", 0)
            extracted_tables = result.get("tables_stored", 0)
            
            logger.info(f"[Task {document_id}] Successfully processed and stored multimodal content")
            
        except Exception as e:
            update_status("Error", f"í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}", 75, total_pages, total_pages, {"error": str(e)})
            logger.error(f"[Task {document_id}] Text processing error: {e}")
            # ì‹¤íŒ¨ ì‹œ íŒŒì¼ ì •ë¦¬
            _cleanup_failed_processing(file_path, document_id)
            return

        # 5. ìµœì¢… ìƒíƒœ ì—…ë°ì´íŠ¸ - ì™„ë£Œ ì²˜ë¦¬
        final_message = f"ì²˜ë¦¬ ì™„ë£Œ! í…ìŠ¤íŠ¸: {text_chunks}ì²­í¬, ì´ë¯¸ì§€: {extracted_images}ê°œ, í‘œ: {extracted_tables}ê°œ"
        final_details = {
            "total_pages": total_pages,
            "text_chunks": text_chunks,
            "images": extracted_images,
            "tables": extracted_tables,
            "text_length": len(extracted_text) if extracted_text else 0,
            "processing_time": None
        }
        update_status("Completed", final_message, 100, total_pages, total_pages, final_details)
        logger.info(f"[Task {document_id}] Successfully processed and stored multimodal content for: {document_id}")
        
        # ì™„ë£Œ í›„ ì¼ì • ì‹œê°„ í›„ ìƒíƒœ ì •ë¦¬ (15ì´ˆ)
        import threading
        def cleanup_status():
            import time
            time.sleep(15)
            if document_id in pdf_processing_status:
                del pdf_processing_status[document_id]
                logger.info(f"[Task {document_id}] Status cleaned up from memory")
        
        cleanup_thread = threading.Thread(target=cleanup_status)
        cleanup_thread.daemon = True
        cleanup_thread.start()

    except Exception as e:
        pdf_processing_status[document_id] = {"step": "Error", "message": f"ì˜ˆì™¸ ë°œìƒ: {str(e)}", "percent": 0}
        logger.error(f"[Task {document_id}] Unexpected error during background PDF processing: {e}", exc_info=True)
        # ì˜ˆì™¸ ë°œìƒ ì‹œ íŒŒì¼ ì •ë¦¬
        _cleanup_failed_processing(file_path, document_id)
    finally:
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        import gc
        gc.collect()

def _cleanup_failed_processing(file_path: str, document_id: str):
    """
    ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ íŒŒì¼ ë° ê´€ë ¨ ë°ì´í„° ì •ë¦¬
    """
    try:
        # 1. ì—…ë¡œë“œëœ íŒŒì¼ ì‚­ì œ
        if os.path.exists(file_path):
            os.remove(file_path)
            logger.info(f"Cleaned up failed upload file: {file_path}")
        
        # 2. ë¶€ë¶„ì ìœ¼ë¡œ ìƒì„±ëœ ì»¨í…ì¸  ë””ë ‰í† ë¦¬ ì •ë¦¬
        content_dir = os.path.join(settings.UPLOAD_DIR, f"{document_id}_content")
        if os.path.exists(content_dir):
            import shutil
            shutil.rmtree(content_dir)
            logger.info(f"Cleaned up content directory: {content_dir}")
        
        # 3. Vector DBì—ì„œ ë¶€ë¶„ì ìœ¼ë¡œ ì €ì¥ëœ ë°ì´í„° ì‚­ì œ
        try:
            delete_multimodal_document(document_id)
            logger.info(f"Cleaned up vector DB data for: {document_id}")
        except Exception as db_error:
            logger.warning(f"Failed to clean up vector DB for {document_id}: {db_error}")
        
        # 4. ì²˜ë¦¬ ìƒíƒœì—ì„œ ì œê±°
        if document_id in pdf_processing_status:
            del pdf_processing_status[document_id]
        
        logger.info(f"Cleanup completed for failed processing: {document_id}")
        
    except Exception as cleanup_error:
        logger.error(f"Error during cleanup for {document_id}: {cleanup_error}")


@router.post("/upload_pdf/")
async def upload_pdf(
    files: List[UploadFile] = File(...),
    ocr_correction_enabled: bool = Form(False),
    llm_correction_enabled: bool = Form(False)
):
    f"""
    ì—¬ëŸ¬ PDF íŒŒì¼ ì—…ë¡œë“œë¥¼ ì§€ì›í•©ë‹ˆë‹¤. ê° íŒŒì¼ì€ ë³‘ë ¬ë¡œ OCR/ì„ë² ë”©/ì €ì¥ ì²˜ë¦¬ë©ë‹ˆë‹¤.
    ìµœëŒ€ {settings.MAX_CONCURRENT_FILE_PROCESSING}ê°œ íŒŒì¼ê¹Œì§€ë§Œ ë™ì‹œ ì²˜ë¦¬ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    """
    # íŒŒì¼ ê°œìˆ˜ ì œí•œ
    if len(files) > settings.MAX_FILES_PER_UPLOAD:
        raise HTTPException(
            status_code=400, 
            detail=f"ìµœëŒ€ {settings.MAX_FILES_PER_UPLOAD}ê°œì˜ íŒŒì¼ê¹Œì§€ë§Œ í•œ ë²ˆì— ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (í˜„ì¬: {len(files)}ê°œ)"
        )
    
    logger.info(f"Processing {len(files)} files (max {settings.MAX_CONCURRENT_FILE_PROCESSING} concurrent)")
    results = []
    for file in files:
        logger.info(f"Upload request received for file: {file.filename}")
        # Basic validation
        if not file.filename:
            results.append({"filename": None, "error": "No filename provided"})
            continue
        # Check file size
        content = await file.read()
        file_size = len(content)
        if file_size == 0:
            results.append({"filename": file.filename, "error": "Empty file uploaded"})
            continue
        # Generate secure document ID and filename
        document_id = f"{os.path.splitext(file.filename)[0]}_{str(uuid.uuid4())[:8]}"
        safe_filename = FileValidator.generate_safe_filename(file.filename, document_id)
        # Save path using pathlib
        file_path = Path(settings.UPLOAD_DIR) / safe_filename
        try:
            # Save file
            with open(file_path, "wb") as buffer:
                buffer.write(content)
            logger.info(f"File saved: {file_path} ({file_size} bytes)")
            # Validate uploaded file
            validation_result = FileValidator.validate_uploaded_file(file_path, file.filename, file_size)
            if not validation_result["is_valid"]:
                os.remove(file_path)
                logger.warning(f"File validation failed: {validation_result['errors']}")
                results.append({"filename": file.filename, "error": f"File validation failed: {'; '.join(validation_result['errors'])}"})
                continue
            # í˜„ì¬ í™œì„± ì‘ì—… ìˆ˜ í™•ì¸
            active_tasks = len([status for status in pdf_processing_status.values() 
                              if status.get("step") not in ["Done", "Completed", "Error", "Queued"]])
            
            # íì—ì„œ ëŒ€ê¸° ì¤‘ì¸ì§€ í™•ì¸
            is_queued = active_tasks >= settings.MAX_CONCURRENT_FILE_PROCESSING
            
            # ì²˜ë¦¬ ìƒíƒœ ì´ˆê¸°í™” (ë°±ê·¸ë¼ìš´ë“œ ì‹œì‘ ì „ì— ë¨¼ì € ì„¤ì •)
            initial_step = "Queued" if is_queued else "Preparing"
            initial_message = "ì²˜ë¦¬ ëŒ€ê¸°ì—´ì—ì„œ ìˆœì„œë¥¼ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘..." if is_queued else "ë¬¸ì„œ ì²˜ë¦¬ ì¤€ë¹„ ì¤‘..."
            
            pdf_processing_status[document_id] = {
                "step": initial_step,
                "message": initial_message,
                "percent": 0,
                "current_page": 0,
                "total_pages": 0,
                "details": {"queued": is_queued, "queue_position": active_tasks + 1 if is_queued else 0},
                "timestamp": __import__('datetime').datetime.now().isoformat()
            }
            logger.info(f"âœ… Processing status initialized for document: {document_id} (queued: {is_queued})")
            
            # Start background processing (ìŠ¤ë ˆë“œ ê¸°ë°˜)
            future = executor.submit(
                process_pdf_background_entry,
                file_path,
                document_id,
                file.filename,
                ocr_correction_enabled,
                llm_correction_enabled
            )
            logger.info(f"Background processing started for document: {document_id}")
            results.append({
                "message": "File uploaded successfully. Processing started in the background.",
                "filename": file.filename,
                "document_id": document_id,
                "file_hash": validation_result["file_hash"],
                "detail": "The PDF is being processed. This may take some time depending on the file size and content."
            })
        except Exception as e:
            if os.path.exists(file_path):
                try:
                    os.remove(file_path)
                    logger.info(f"Cleaned up file after error: {file_path}")
                except OSError as cleanup_error:
                    logger.error(f"Error cleaning up file {file_path}: {cleanup_error}")
            logger.error(f"Unexpected error during file upload: {e}")
            results.append({"filename": file.filename, "error": "Could not save or start processing file"})
    return JSONResponse(content={"results": results}, status_code=202)


@router.get("/health")
async def health_check():
    """Health check endpoint for monitoring"""
    monitor = get_monitor()
    health_status = monitor.check_health()
    
    status_code = 200 if health_status["status"] == "healthy" else 503
    return JSONResponse(content=health_status, status_code=status_code)

@router.get("/metrics")
async def get_metrics():
    """Get application performance metrics"""
    monitor = get_monitor()
    stats = monitor.get_stats()
    
    return JSONResponse(content={
        "performance": stats,
        "timestamp": time.time()
    })

@router.post("/chat/stream/")
async def chat_with_documents_stream(request: ChatRequest):
    """
    Process a chat query with streaming response for real-time user experience.
    """
    query = request.query
    model_name = request.model_name
    lang = request.lang or "ko"
    conversation_history = request.conversation_history or []
    logger.info(f"Stream request - conversation history: {len(conversation_history)} messages")
    
    # Support both single document_id and multiple document_ids
    document_ids = []
    if request.document_ids:
        document_ids = request.document_ids
    elif request.document_id:
        document_ids = [request.document_id]
    

    # Process real-time streaming
    async def stream_response():
        import json
        
        try:
            # 1. Query embedding
            yield f"data: {json.dumps({'type': 'status', 'message': 'ì§ˆë¬¸ ë¶„ì„ ì¤‘...'}, ensure_ascii=False)}\n\n"
            
            import time
            start_time = time.time()
            
            # ì§ˆë¬¸ ìœ íš¨ì„± ê²€ì¦
            validation_result = QueryValidator.validate_query(query)
            if not validation_result['is_valid']:
                error_response = {
                    'type': 'validation_error',
                    'message': validation_result['suggestion'],
                    'suggestions': QueryValidator.get_query_suggestions(query)
                }
                yield f"data: {json.dumps(error_response, ensure_ascii=False)}\n\n"
                return
            
            try:
                # ê²€ìƒ‰ì„ ìœ„í•œ ì§ˆë¬¸ í–¥ìƒ
                enhanced_query = QueryValidator.enhance_query_for_search(query)
                query_embeddings = get_embeddings([enhanced_query])
                if not query_embeddings:
                    fallback_data = FallbackResponseService.generate_error_response(
                        "embedding_error", "ì„ë² ë”© ìƒì„± ì‹¤íŒ¨", query
                    )
                    yield f"data: {json.dumps({'type': 'error', 'message': fallback_data}, ensure_ascii=False)}\n\n"
                    return
                query_vector = query_embeddings[0]
                embedding_time = time.time() - start_time
                logger.info(f"Query embedding generation took: {embedding_time:.2f} seconds")
            except Exception as e:
                logger.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
                fallback_data = FallbackResponseService.generate_error_response(
                    "embedding_error", str(e), query
                )
                yield f"data: {json.dumps({'type': 'error', 'message': fallback_data}, ensure_ascii=False)}\n\n"
                return
            
            # 2. Vector search
            yield f"data: {json.dumps({'type': 'status', 'message': 'ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...'}, ensure_ascii=False)}\n\n"
            
            try:
                # Apply document filter if specified
                filter_metadata = None
                if document_ids and len(document_ids) > 0:
                    if len(document_ids) == 1:
                        filter_metadata = {"source_document_id": document_ids[0]}
                    else:
                        filter_metadata = {"source_document_id": {"$in": document_ids}}
                
                multimodal_results = search_multimodal_content(
                    query_vector=query_vector,
                    top_k=settings.TOP_K_RESULTS,
                    filter_metadata=filter_metadata,
                    include_images=True,
                    include_tables=True
                )
            except Exception as e:
                logger.error(f"ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                yield f"data: {json.dumps({'type': 'error', 'message': f'ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'}, ensure_ascii=False)}\n\n"
                return
            
            # Debug: Log search results
            retrieved_chunks = multimodal_results.get('text', [])
            retrieved_images = multimodal_results.get('images', [])
            retrieved_tables = multimodal_results.get('tables', [])
            
            logger.info(f"Streaming search results: text={len(retrieved_chunks)}, images={len(retrieved_images)}, tables={len(retrieved_tables)}")
            
            # Check if we have any search results
            if not retrieved_chunks and not retrieved_images and not retrieved_tables:
                fallback_data = FallbackResponseService.generate_no_results_response(query, document_ids)
                yield f"data: {json.dumps({'type': 'no_results', 'message': fallback_data['response'], 'suggestions': fallback_data['suggestions']}, ensure_ascii=False)}\n\n"
                return
            
            # Debug: Log actual content
            if retrieved_chunks:
                logger.info(f"First text chunk: {retrieved_chunks[0].get('text', '')[:100]}...")
            if retrieved_tables:
                logger.info(f"First table content: {retrieved_tables[0].get('content', '')[:100]}...")
            
            # 3. Stream LLM response
            yield f"data: {json.dumps({'type': 'status', 'message': 'ë‹µë³€ ìƒì„± ì¤‘...'}, ensure_ascii=False)}\n\n"
            
            all_retrieved_content = {
                'text': retrieved_chunks,
                'images': retrieved_images,
                'tables': retrieved_tables
            }
            
            # Get streaming response from LLM
            llm_options = {
                "num_predict": settings.LLM_NUM_PREDICT_MULTIMODAL,
                "temperature": settings.LLM_TEMPERATURE,
                "top_p": 0.9,
                "repeat_penalty": 1.1
            }
            
            full_response = ""
            word_buffer = ""
            
            try:
                stream_generator = process_multimodal_llm_chat_request_stream(
                    user_query=query,
                    multimodal_content=all_retrieved_content,
                    model_name=model_name,
                    lang=lang,
                    options=llm_options,
                    conversation_history=conversation_history
                )
            except Exception as e:
                logger.error(f"LLM ìŠ¤íŠ¸ë¦¬ë° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                yield f"data: {json.dumps({'type': 'error', 'message': f'ë‹µë³€ ìƒì„± ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'}, ensure_ascii=False)}\n\n"
                return
            
            for chunk in stream_generator:
                full_response += chunk
                word_buffer += chunk
                
                # Send complete words/phrases instead of single characters (Korean-optimized)
                korean_delimiters = [' ', '\n', '.', ',', '!', '?', ')', ']', '}', 'ë‹¤.', 'ë‹¤,', 'ë‹¤!', 'ë‹¤?', 'ìš”.', 'ìš”,', 'ìš”!', 'ìš”?']
                if any(delimiter in word_buffer for delimiter in korean_delimiters) or len(word_buffer) > 50:
                    chunk_data = {
                        "type": "content",
                        "content": word_buffer,
                        "is_final": False
                    }
                    yield f"data: {json.dumps(chunk_data, ensure_ascii=False)}\n\n"
                    word_buffer = ""
            
            # Send any remaining content
            if word_buffer:
                chunk_data = {
                    "type": "content",
                    "content": word_buffer,
                    "is_final": False
                }
                yield f"data: {json.dumps(chunk_data, ensure_ascii=False)}\n\n"
            
            # Enhance and cache the complete response
            enhanced_response = enhance_response_with_media_references(
                full_response,
                retrieved_images,
                retrieved_tables
            )
            
            # Validate and enhance response
            final_response_text = enhanced_response.get('text', full_response) if enhanced_response else full_response
            if not final_response_text or not final_response_text.strip():
                fallback_data = FallbackResponseService.generate_no_results_response(query, document_ids)
                final_response_text = fallback_data['response']
            elif len(final_response_text.strip()) < 50:
                # ì‘ë‹µì´ ë„ˆë¬´ ì§§ì€ ê²½ìš° í–¥ìƒ
                total_results = len(retrieved_chunks) + len(retrieved_images) + len(retrieved_tables)
                final_response_text = FallbackResponseService.enhance_poor_results_response(
                    final_response_text, query, total_results
                )
            
            response_data = {
                'query': query,
                'response': final_response_text,
                'source_document_id': document_ids[0] if document_ids else None,
                'retrieved_chunks_preview': [chunk.get('text', '')[:100] + "..." for chunk in retrieved_chunks if chunk.get('text')],
                'content_summary': {
                    'text_chunks': len(retrieved_chunks),
                    'images': len(retrieved_images),
                    'tables': len(retrieved_tables)
                },
                'media_references': {
                    'images': enhanced_response.get('referenced_images', []) if enhanced_response else [],
                    'tables': enhanced_response.get('referenced_tables', []) if enhanced_response else [],
                    'has_media': enhanced_response.get('has_media', False) if enhanced_response else False
                }
            }
            
            
            # Send final metadata
            final_data = {
                "type": "final",
                "metadata": {
                    "content_summary": response_data['content_summary'],
                    "media_references": response_data['media_references'],
                }
            }
            yield f"data: {json.dumps(final_data, ensure_ascii=False)}\n\n"
            
        except Exception as e:
            logger.error(f"Streaming chat error: {e}", exc_info=True)
            fallback_message = FallbackResponseService.generate_error_response(
                "system_error", str(e), query
            )
            error_data = {
                "type": "error",
                "message": fallback_message
            }
            yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"
    
    return StreamingResponse(
        stream_response(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )

@router.post("/chat/", response_model=ChatResponse)
async def chat_with_llm(chat_request: ChatRequest):
    """
    Handles chat requests with input validation and security checks.
    1. Validates and sanitizes user input
    2. Embeds the user query
    3. Searches for relevant chunks in the Vector DB (optionally filtered by document_id)
    4. Constructs a RAG prompt and gets a response from the LLM
    """
    # Input validation and sanitization
    query = sanitize_input(chat_request.query, max_length=2000)
    if not query:
        raise HTTPException(status_code=400, detail="Query not provided or invalid.")
    
    # Extract conversation history
    conversation_history = chat_request.conversation_history or []
    logger.info(f"Received conversation history: {len(conversation_history)} messages")
    
    # ì§ˆë¬¸ ìœ íš¨ì„± ê²€ì¦
    validation_result = QueryValidator.validate_query(query)
    if not validation_result['is_valid']:
        suggestions = QueryValidator.get_query_suggestions(query)
        raise HTTPException(
            status_code=400, 
            detail={
                "message": validation_result['suggestion'],
                "error_type": validation_result['error_type'],
                "suggestions": suggestions
            }
        )
    
    # Validate document IDs
    document_ids = chat_request.document_ids or ([] if chat_request.document_id is None else [chat_request.document_id])
    if document_ids:
        for doc_id in document_ids:
            if not validate_document_id(doc_id):
                raise HTTPException(status_code=400, detail=f"Invalid document ID: {doc_id}")
    
    model_name = chat_request.model_name or settings.OLLAMA_DEFAULT_MODEL
    lang = chat_request.lang or "ko"

    logger.info(f"Chat request: Query length={len(query)}, DocIDs={len(document_ids)}, Model={model_name}, Lang={lang}")
    logger.debug(f"Query preview: {query[:100]}...")

    try:
        # 1. Embed user query (with enhancement)
        logger.info("Step 1: Embedding user query...")
        enhanced_query = QueryValidator.enhance_query_for_search(query)
        query_embedding_list = get_embeddings([enhanced_query])
        if not query_embedding_list or not query_embedding_list[0]:
            raise EmbeddingError("Could not generate embedding for the query", "QUERY_EMBEDDING_FAILED")
        query_embedding = query_embedding_list[0]

        # 2. Search Multimodal Content
        logger.info("Step 2: Searching multimodal content for relevant information...")
        filter_metadata = None
        if document_ids and len(document_ids) > 0:
            if len(document_ids) == 1:
                filter_metadata = {"source_document_id": document_ids[0]}
            else:
                filter_metadata = {"source_document_id": {"$in": document_ids}}
            logger.debug(f"Applying filter: {filter_metadata}")
        
        # Search across all content types
        multimodal_results = search_multimodal_content(
            query_vector=query_embedding, 
            top_k=5, 
            filter_metadata=filter_metadata,
            include_images=True,
            include_tables=True
        )
        
        # Extract text results for backward compatibility
        retrieved_chunks = multimodal_results.get('text', [])
        retrieved_images = multimodal_results.get('images', [])
        retrieved_tables = multimodal_results.get('tables', [])

        if not any([retrieved_chunks, retrieved_images, retrieved_tables]):
            logger.warning("No relevant content found in Vector DB")
            # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° ëŒ€ì²´ ì‘ë‹µ ìƒì„±
            fallback_data = FallbackResponseService.generate_no_results_response(query, document_ids)
            return ChatResponse(
                query=query,
                response=fallback_data['response'],
                source_document_id=document_ids[0] if document_ids else None,
                retrieved_chunks_preview=[],
                content_summary={'text_chunks': 0, 'images': 0, 'tables': 0},
                media_references={'images': [], 'tables': [], 'has_media': False}
            )

        retrieved_chunk_texts_preview = [chunk.get('text', '')[:100] + "..." for chunk in retrieved_chunks]

        # 3. Get LLM response via Multimodal RAG
        logger.info("Step 3: Getting LLM response using multimodal RAG...")
        
        # Combine all retrieved content for LLM processing
        all_retrieved_content = {
            'text_chunks': retrieved_chunks,
            'images': retrieved_images,
            'tables': retrieved_tables
        }
        
        # Use multimodal LLM service with optimized options
        llm_options = {
            "num_predict": settings.LLM_NUM_PREDICT_MULTIMODAL,
            "temperature": settings.LLM_TEMPERATURE,
            "top_p": 0.9,  # ì‘ë‹µ ì¼ê´€ì„± í–¥ìƒ
            "repeat_penalty": 1.1  # ë°˜ë³µ ë°©ì§€
        }
        
        llm_response_text = process_multimodal_llm_chat_request(
            user_query=query,
            multimodal_content=all_retrieved_content,
            model_name=model_name,
            lang=lang,
            options=llm_options,
            conversation_history=conversation_history
        )
        
        # Enhance response with media references for UI display
        enhanced_response = enhance_response_with_media_references(
            llm_response_text,
            retrieved_images,
            retrieved_tables
        )
    
    except EmbeddingError as e:
        logger.error(f"Embedding error in chat: {e}")
        fallback_message = FallbackResponseService.generate_error_response("embedding_error", e.message, query)
        raise HTTPException(status_code=500, detail=fallback_message)
    except VectorDBError as e:
        logger.error(f"Vector DB error in chat: {e}")
        fallback_message = FallbackResponseService.generate_error_response("vector_error", e.message, query)
        raise HTTPException(status_code=500, detail=fallback_message)
    except LLMError as e:
        logger.error(f"LLM error in chat: {e}")
        fallback_message = FallbackResponseService.generate_error_response("llm_error", e.message, query)
        raise HTTPException(status_code=500, detail=fallback_message)
    except Exception as e:
        logger.error(f"Unexpected error in chat: {e}", exc_info=True)
        fallback_message = FallbackResponseService.generate_error_response("system_error", str(e), query)
        raise HTTPException(status_code=500, detail=fallback_message)

    # Enhanced response with multimodal content info and fallback handling
    final_response_text = enhanced_response.get('text', llm_response_text)
    
    # ì‘ë‹µ í’ˆì§ˆ ê²€ì¦ ë° í–¥ìƒ
    if not final_response_text or len(final_response_text.strip()) < 30:
        total_results = len(retrieved_chunks) + len(retrieved_images) + len(retrieved_tables)
        if total_results == 0:
            fallback_data = FallbackResponseService.generate_no_results_response(query, document_ids)
            final_response_text = fallback_data['response']
        else:
            final_response_text = FallbackResponseService.enhance_poor_results_response(
                final_response_text, query, total_results
            )
    
    response_data = {
        'query': query,
        'response': final_response_text,
        'source_document_id': document_ids[0] if document_ids else None,
        'retrieved_chunks_preview': retrieved_chunk_texts_preview,
        'content_summary': {
            'text_chunks': len(retrieved_chunks),
            'images': len(retrieved_images),
            'tables': len(retrieved_tables)
        },
        'media_references': {
            'images': enhanced_response.get('referenced_images', []),
            'tables': enhanced_response.get('referenced_tables', []),
            'has_media': enhanced_response.get('has_media', False)
        }
    }
    
    
    return ChatResponse(**response_data)

@router.get("/upload_status/{document_id}")
def get_upload_status(document_id: str):
    status = pdf_processing_status.get(document_id)
    if status:
        logger.debug(f"ğŸ“Š Status check for {document_id}: {status['step']} - {status.get('percent', 0)}%")
        return status
    else:
        # ë” ìƒì„¸í•œ ë””ë²„ê¹… ì •ë³´ ì œê³µ
        available_ids = list(pdf_processing_status.keys())
        logger.warning(f"âš ï¸ Status not found for document: {document_id}")
        logger.warning(f"ğŸ“‹ Available document IDs ({len(available_ids)}): {available_ids}")
        
        # ë¹„ìŠ·í•œ IDê°€ ìˆëŠ”ì§€ í™•ì¸ (ì˜¤íƒ€ë‚˜ ID ë³€ê²½ ê°ì§€)
        similar_ids = [id for id in available_ids if document_id in id or id in document_id]
        if similar_ids:
            logger.info(f"ğŸ” Similar IDs found: {similar_ids}")
        
        # ì˜¤ë˜ëœ ìƒíƒœ ì •ë³´ ì •ë¦¬ (1ì‹œê°„ ì´ìƒ ëœ ê²ƒ)
        import datetime
        current_time = datetime.datetime.now()
        cleaned_count = 0
        
        for doc_id, doc_status in list(pdf_processing_status.items()):
            try:
                timestamp_str = doc_status.get('timestamp')
                if timestamp_str:
                    timestamp = datetime.datetime.fromisoformat(timestamp_str.replace('Z', '+00:00').replace('+00:00', ''))
                    age_minutes = (current_time - timestamp).total_seconds() / 60
                    
                    # 1ì‹œê°„ ì´ìƒ ëœ ì™„ë£Œ/ì—ëŸ¬ ìƒíƒœëŠ” ì •ë¦¬
                    if age_minutes > 60 and doc_status.get('step') in ['Done', 'Completed', 'Error']:
                        del pdf_processing_status[doc_id]
                        cleaned_count += 1
                        logger.info(f"ğŸ§¹ Cleaned old status for {doc_id} (age: {age_minutes:.1f} minutes)")
            except Exception as e:
                logger.debug(f"Failed to parse timestamp for {doc_id}: {e}")
        
        if cleaned_count > 0:
            logger.info(f"ğŸ§¹ Cleaned {cleaned_count} old status entries")
        
        return {
            "step": "Unknown", 
            "message": f"í•´ë‹¹ ë¬¸ì„œì˜ ìƒíƒœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ID: {document_id})",
            "percent": 0,
            "details": {
                "available_count": len(pdf_processing_status),
                "requested_id": document_id,
                "similar_ids": similar_ids,
                "cleaned_old_entries": cleaned_count,
                "debug_info": f"ë°±ì—”ë“œì—ì„œ {len(available_ids)}ê°œ ë¬¸ì„œ ìƒíƒœë¥¼ ì¶”ì  ì¤‘"
            }
        }

@router.get("/ollama/status")
def ollama_status():
    """
    Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    """
    # Check Ollama server availability: try /api/models then fallback to /api/tags
    endpoints = []
    base = settings.OLLAMA_API_URL
    endpoints.append(base.replace('/api/generate', '/api/models'))
    endpoints.append(base.replace('/api/generate', '/api/tags'))
    last_detail = None
    for url in endpoints:
        try:
            resp = requests.get(url, timeout=settings.OLLAMA_TIMEOUT)
            if resp.status_code == 200:
                return {"status": "running", "endpoint": url}
            last_detail = resp.text
        except Exception as e:
            last_detail = str(e)
            continue
    # All attempts failed
    return {
        "status": "unreachable",
        "detail": last_detail or "no response",
        "hint": "Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš” (ì˜ˆ: 'ollama serve') ë˜ëŠ” OLLAMA_API_URL ì„¤ì •ì„ ê²€í† í•˜ì„¸ìš”."
    }

@router.get("/ollama/models")
def ollama_models(force_refresh: bool = False, quick: bool = True):
    """
    Ollamaì— ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    ìºì‹±ì„ í†µí•´ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê³ , í•„ìš”ì‹œì—ë§Œ LLMìœ¼ë¡œë¶€í„° ëª¨ë¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    
    Args:
        force_refresh: ìºì‹œë¥¼ ë¬´ì‹œí•˜ê³  ê°•ì œë¡œ ìƒˆë¡œê³ ì¹¨í• ì§€ ì—¬ë¶€
        quick: ë¹ ë¥¸ ì‘ë‹µì„ ìœ„í•´ ê¸°ë³¸ ëª¨ë¸ ëª©ë¡ ë°˜í™˜ (ê¸°ë³¸ê°’: True)
    """
    if quick:
        # ë¹ ë¥¸ ì‘ë‹µì„ ìœ„í•´ í´ë°± ë¨¼ì € ì‹œë„
        try:
            fallback_result = _fallback_model_list()
            if fallback_result.get("models"):
                logger.info(f"Quick response: returning {len(fallback_result['models'])} models")
                return fallback_result
        except Exception as e:
            logger.warning(f"Quick fallback failed: {e}")
    
    try:
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ìºì‹œ ì—…ë°ì´íŠ¸ ì‹œë„ (ë¹„ë™ê¸°ì ìœ¼ë¡œ)
        import threading
        
        def background_cache_update():
            try:
                from app.services.model_info_service import get_cached_model_info
                get_cached_model_info(force_refresh=force_refresh)
                logger.info("Background model info cache update completed")
            except Exception as e:
                logger.error(f"Background cache update failed: {e}")
        
        if not quick:
            # ì¦‰ì‹œ ëª¨ë“œì—ì„œëŠ” ì§ì ‘ ìºì‹œ í™•ì¸
            from app.services.model_info_service import get_cached_model_info
            
            logger.info(f"Fetching model info (force_refresh={force_refresh})")
            model_data = get_cached_model_info(force_refresh=force_refresh)
            
            if model_data.get("models"):
                logger.info(f"Returning {len(model_data['models'])} models with cached info")
                return model_data
        else:
            # í€µ ëª¨ë“œì—ì„œëŠ” ë°±ê·¸ë¼ìš´ë“œ ì—…ë°ì´íŠ¸ë§Œ ì‹œì‘
            if not force_refresh:  # ê°•ì œ ìƒˆë¡œê³ ì¹¨ì´ ì•„ë‹ ë•Œë§Œ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
                thread = threading.Thread(target=background_cache_update, daemon=True)
                thread.start()
        
        # í´ë°±: ê¸°ë³¸ ëª¨ë¸ ëª©ë¡ ë°˜í™˜
        return _fallback_model_list()
        
    except Exception as e:
        logger.error(f"Error fetching model info: {e}")
        # ìµœì¢… í´ë°±: ê¸°ë³¸ ëª¨ë¸ ëª©ë¡ë§Œ ë°˜í™˜
        return _fallback_model_list()

def _fallback_model_list():
    """í´ë°±: ê¸°ë³¸ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
    # Try listing via /api/models then fallback to /api/tags
    base = settings.OLLAMA_API_URL
    urls = [
        base.replace('/api/generate', '/api/models'),
        base.replace('/api/generate', '/api/tags'),
    ]
    raw_models = []
    detail = None
    for url in urls:
        try:
            resp = requests.get(url, timeout=settings.OLLAMA_TIMEOUT)
            if resp.status_code != 200:
                detail = resp.text
                continue
            data = resp.json()
            # parse possible response structures
            if isinstance(data, dict) and 'models' in data:
                mlist = data.get('models') or []
                if mlist and isinstance(mlist[0], dict) and 'name' in mlist[0]:
                    raw_models = [m.get('name', '') for m in mlist]
                else:
                    raw_models = list(mlist)
            elif isinstance(data, list):
                raw_models = data
            else:
                raw_models = []
            break
        except Exception as e:
            detail = str(e)
            continue
    else:
        # all attempts failed
        return {
            "models": [],
            "summaries": [],
            "detail": detail or 'no response',
            "hint": "Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš” (ì˜ˆ: 'ollama serve') ë˜ëŠ” OLLAMA_API_URL ì„¤ì •ì„ ê²€í† í•˜ì„¸ìš”."
        }
    # Summaries for known models
    MODEL_SUMMARIES = {
        "seokdong-llama-3.1-8b": {"display_name": "SEOKDONG-Llama 3.1 (8B)",
            "provider": "Kwangsuklee Â· Meta ê¸°ë°˜", "params": "8 B (Q5_K_M ì–‘ìí™” â‰ˆ 5.7 GB)",
            "key_points": "í•œêµ­ì–´ íŠ¹í™” SFT, 128 K ì»¨í…ìŠ¤íŠ¸, 2025ë…„ éŸ“ LLM ë¦¬ë”ë³´ë“œ 6ìœ„"},
        "sfr-embedding-mistral-7b": {"display_name": "SFR-Embedding (Mistral-7B)",
            "provider": "Salesforce Research", "params": "7 B",
            "key_points": "E5-Mistral-7B ê¸°ë°˜ ì„ë² ë”© ì „ìš© ëª¨ë¸, MTEB ìƒìœ„ê¶Œ ì„±ëŠ¥"},
        "llama-3.1-8b": {"display_name": "Llama 3.1 (8B)",
            "provider": "Meta", "params": "8 B",
            "key_points": "ë‹¤êµ­ì–´ ì§€ì›, 128 K ì»¨í…ìŠ¤íŠ¸, ë²”ìš© ë² ì´ìŠ¤ ëª¨ë¸"},
        "qwen-3-235b-a22b": {"display_name": "Qwen 3 235B-A22B", 
            "provider": "Alibaba Cloud", "params": "235 B ì´ (í™œì„± 22 B MoE)",
            "key_points": "94-ë ˆì´ì–´ MoEÂ·GQA êµ¬ì¡°, 32 Kâ†’131 K ì»¨í…ìŠ¤íŠ¸ í™•ì¥, ê³ ë‚œë„ ì¶”ë¡ Â·ì½”ë”© ê°•ì "},
        "llama-3.2-3b-instruct": {"display_name": "Llama 3.2 (3B Instruct)",
            "provider": "Meta", "params": "3.2 B (Q5_K_M)",
            "key_points": "ì†Œí˜• ë©€í‹°ë§ê¶ ì§€ì‹œì‘ë‹µ íŠ¹í™”, 1 B/3 B ë²„ì „, ìš”ì•½Â·íˆ´ì‚¬ìš© íŠœë‹"},
        "qwen-3-32b": {"display_name": "Qwen 3 32B", 
            "provider": "Alibaba Cloud", "params": "32.8 B",
            "key_points": "â€˜Thinking â†” Non-Thinkingâ€™ í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ, 128 K ì»¨í…ìŠ¤íŠ¸, ì—ì´ì „íŠ¸Â·ì¶”ë¡  ê°•í™”"},
        "gemma-3-27b-qat": {"display_name": "Gemma 3 27B-QAT", 
            "provider": "Google", "params": "27.4 B",
            "key_points": "ë©€í‹°ëª¨ë‹¬(í…ìŠ¤íŠ¸+ì´ë¯¸ì§€), 128 K ì»¨í…ìŠ¤íŠ¸, QAT ë•ë¶„ì— ë‹¨ì¼ GPU êµ¬ë™"},
        "deepseek-r1-distill-32b": {"display_name": "DeepSeek-R1 Distill 32B", 
            "provider": "DeepSeek", "params": "32 B",
            "key_points": "RL-ê¸°ë°˜ Reasoning R1 ëª¨ë¸ì„ ì†Œí˜•í™”, 32 K ì»¨í…ìŠ¤íŠ¸, o1-miniê¸‰ ì„±ëŠ¥"},
        "qwq-32b": {"display_name": "QwQ 32B", 
            "provider": "Alibaba Cloud", "params": "32.5 B",
            "key_points": "ê°•í™”í•™ìŠµ Reasoning íŠ¹í™”, 131 K ì»¨í…ìŠ¤íŠ¸, DeepSeek-R1ê³¼ ê²½ìŸ"}
    }
    MODEL_TIPS = [
        "ì¶”ë¡ Â·ì½”ë”© ë¬¸ì œ í•´ê²° â†’ Qwen 3 235B, Qwen 3 32B, QwQ 32B, DeepSeek-R1 Distill 32B.",
        "í•œêµ­ì–´ ì§ˆì˜ì‘ë‹µ â†’ SEOKDONG-Llama 3.1 (8B).",
        "RAGÂ·ê²€ìƒ‰ ì„ë² ë”© â†’ SFR-Embedding (Mistral-7B).",
        "ë¦¬ì†ŒìŠ¤ ì œì•½ í™˜ê²½ â†’ Llama 3.2 (3B Instruct), Gemma 3 27B-QAT (ë‹¨ì¼ GPU ê°€ëŠ¥)."
    ]
    # Prepare summaries for enriched info
    enriched = []
    for raw in raw_models:
        key = raw.lower()
        info = MODEL_SUMMARIES.get(key)
        entry = {"name": raw}
        if info:
            entry.update(info)
        else:
            entry.update({"display_name": raw, "provider": "", "params": "", "key_points": ""})
        enriched.append(entry)
    # Return raw model names for dropdown, enriched summaries for detailed table, and usage tips
    return {"models": raw_models, "summaries": enriched, "tips": MODEL_TIPS}

@router.post("/ollama/models/refresh")
def refresh_model_info(model_name: str = None):
    """
    ëª¨ë¸ ì •ë³´ ìºì‹œë¥¼ ìˆ˜ë™ìœ¼ë¡œ ìƒˆë¡œê³ ì¹¨í•©ë‹ˆë‹¤.
    
    Args:
        model_name: íŠ¹ì • ëª¨ë¸ë§Œ ìƒˆë¡œê³ ì¹¨í•  ê²½ìš° ëª¨ë¸ ì´ë¦„ (ì„ íƒì‚¬í•­)
    """
    try:
        from app.services.model_info_service import refresh_model_cache
        
        if model_name:
            logger.info(f"Refreshing cache for specific model: {model_name}")
            refresh_model_cache(model_name)
            return {"message": f"ëª¨ë¸ '{model_name}' ì •ë³´ê°€ ìƒˆë¡œê³ ì¹¨ë˜ì—ˆìŠµë‹ˆë‹¤."}
        else:
            logger.info("Refreshing cache for all models")
            refresh_model_cache()
            return {"message": "ëª¨ë“  ëª¨ë¸ ì •ë³´ê°€ ìƒˆë¡œê³ ì¹¨ë˜ì—ˆìŠµë‹ˆë‹¤."}
            
    except Exception as e:
        logger.error(f"Error refreshing model cache: {e}")
        raise HTTPException(status_code=500, detail=f"ëª¨ë¸ ì •ë³´ ìƒˆë¡œê³ ì¹¨ ì‹¤íŒ¨: {str(e)}")

@router.get("/documents")
def list_documents():
    """
    DBì— ì €ì¥ëœ ëª¨ë“  ë¬¸ì„œ(document_id, chunk ê°œìˆ˜, ë¯¸ë¦¬ë³´ê¸° ë“±) ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        docs = get_all_documents()
        return {"documents": docs}
    except VectorDBError as e:
        logger.error(f"Error listing documents: {e}")
        raise HTTPException(status_code=500, detail=f"ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì˜¤ë¥˜: {e.message}")

@router.delete("/documents/{document_id}")
def delete_document_by_id(document_id: str):
    """
    íŠ¹ì • ë¬¸ì„œ IDì— í•´ë‹¹í•˜ëŠ” ë¬¸ì„œë¥¼ DBì™€ íŒŒì¼ ì‹œìŠ¤í…œì—ì„œ ì‚­ì œí•©ë‹ˆë‹¤.
    """
    if not validate_document_id(document_id):
        raise HTTPException(status_code=400, detail=f"Invalid document ID: {document_id}")
    
    logger.info(f"Delete request for document: {document_id}")
    
    try:
        # 1. ë²¡í„° DBì—ì„œ ë©€í‹°ëª¨ë‹¬ ì»¨í…íŠ¸ ì‚­ì œ
        db_deleted = delete_multimodal_document(document_id)
        
        # 2. íŒŒì¼ ì‹œìŠ¤í…œì—ì„œ ì‚­ì œ
        file_deleted = DocumentFileManager.delete_file_by_document_id(document_id)
        
        # 3. ì²˜ë¦¬ ìƒíƒœì—ì„œë„ ì œê±°
        if document_id in pdf_processing_status:
            del pdf_processing_status[document_id]
        
        if db_deleted or file_deleted:
            logger.info(f"Successfully deleted document: {document_id} (DB: {db_deleted}, File: {file_deleted})")
            return {
                "message": f"Document {document_id} deleted successfully",
                "document_id": document_id,
                "deleted_from_db": db_deleted,
                "deleted_from_files": file_deleted
            }
        else:
            logger.warning(f"Document not found: {document_id}")
            raise HTTPException(status_code=404, detail=f"Document {document_id} not found")
            
    except VectorDBError as e:
        logger.error(f"Vector DB error during delete: {e}")
        raise HTTPException(status_code=500, detail=f"DB ì‚­ì œ ì˜¤ë¥˜: {e.message}")
    except FileProcessingError as e:
        logger.error(f"File processing error during delete: {e}")
        raise HTTPException(status_code=500, detail=f"íŒŒì¼ ì‚­ì œ ì˜¤ë¥˜: {e.message}")
    except Exception as e:
        logger.error(f"Unexpected error during delete: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="ë¬¸ì„œ ì‚­ì œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤")

@router.delete("/documents")
def delete_all_documents_endpoint():
    """
    ëª¨ë“  ë¬¸ì„œë¥¼ DBì™€ íŒŒì¼ ì‹œìŠ¤í…œì—ì„œ ì‚­ì œí•©ë‹ˆë‹¤.
    """
    logger.info("Delete all documents request received")
    
    try:
        # 1. ê¸°ë³¸ ë²¡í„° DBì—ì„œ ëª¨ë“  ë¬¸ì„œ ì‚­ì œ
        db_deleted_count = delete_all_documents()
        
        # 2. ë©€í‹°ëª¨ë‹¬ ì»¬ë ‰ì…˜ì—ì„œ ëª¨ë“  ë¬¸ì„œ ì‚­ì œ
        multimodal_deleted_count = delete_all_multimodal_documents()
        
        # 3. íŒŒì¼ ì‹œìŠ¤í…œì—ì„œ ëª¨ë“  íŒŒì¼ ì‚­ì œ
        file_deleted_count = DocumentFileManager.delete_all_files()
        
        # 4. ì²˜ë¦¬ ìƒíƒœ ì´ˆê¸°í™”
        pdf_processing_status.clear()
        
        logger.info(f"Successfully deleted all documents (Text DB: {db_deleted_count} docs, Multimodal DB: {multimodal_deleted_count} docs, Files: {file_deleted_count} files)")
        
        return {
            "message": "All documents deleted successfully",
            "deleted_documents_count": max(db_deleted_count, multimodal_deleted_count),
            "deleted_files_count": file_deleted_count,
            "text_db_count": db_deleted_count,
            "multimodal_db_count": multimodal_deleted_count
        }
        
    except VectorDBError as e:
        logger.error(f"Vector DB error during delete all: {e}")
        raise HTTPException(status_code=500, detail=f"DB ì „ì²´ ì‚­ì œ ì˜¤ë¥˜: {e.message}")
    except FileProcessingError as e:
        logger.error(f"File processing error during delete all: {e}")
        raise HTTPException(status_code=500, detail=f"íŒŒì¼ ì „ì²´ ì‚­ì œ ì˜¤ë¥˜: {e.message}")
    except Exception as e:
        logger.error(f"Unexpected error during delete all: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="ì „ì²´ ë¬¸ì„œ ì‚­ì œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤")

@router.get("/documents/{document_id}")
def get_document_details(document_id: str):
    """
    íŠ¹ì • ë¬¸ì„œì˜ ìƒì„¸ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if not validate_document_id(document_id):
        raise HTTPException(status_code=400, detail=f"Invalid document ID: {document_id}")
    
    try:
        # DBì—ì„œ ë©€í‹°ëª¨ë‹¬ ë¬¸ì„œ ì •ë³´ ì¡°íšŒ
        db_info = get_multimodal_document_info(document_id)
        if not db_info:
            raise HTTPException(status_code=404, detail=f"Document {document_id} not found in database")
        
        # íŒŒì¼ ì‹œìŠ¤í…œì—ì„œ íŒŒì¼ ì •ë³´ ì¡°íšŒ
        file_info = DocumentFileManager.get_file_info(document_id)
        
        # ì •ë³´ í†µí•©
        result = {
            "document_id": document_id,
            "db_info": db_info,
            "file_info": file_info,
            "processing_status": pdf_processing_status.get(document_id)
        }
        
        return result
        
    except HTTPException as e:
        raise e
    except VectorDBError as e:
        logger.error(f"Vector DB error getting document details: {e}")
        raise HTTPException(status_code=500, detail=f"ë¬¸ì„œ ì •ë³´ ì¡°íšŒ ì˜¤ë¥˜: {e.message}")
    except FileProcessingError as e:
        logger.error(f"File processing error getting document details: {e}")
        raise HTTPException(status_code=500, detail=f"íŒŒì¼ ì •ë³´ ì¡°íšŒ ì˜¤ë¥˜: {e.message}")

@router.post("/documents/cleanup")
def cleanup_orphaned_files():
    """
    ë²¡í„° DBì— ì—†ëŠ” ê³ ì•„ íŒŒì¼ë“¤ì„ ì •ë¦¬í•©ë‹ˆë‹¤.
    """
    logger.info("Cleanup orphaned files request received")
    
    try:
        # 1. ë²¡í„° DBì—ì„œ ìœ íš¨í•œ document_id ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        documents = get_all_documents()
        valid_document_ids = [doc["document_id"] for doc in documents]
        
        # 2. ê³ ì•„ íŒŒì¼ ì •ë¦¬
        orphaned_count = DocumentFileManager.cleanup_orphaned_files(valid_document_ids)
        
        logger.info(f"Cleaned up {orphaned_count} orphaned files")
        
        return {
            "message": "Orphaned files cleanup completed",
            "cleaned_files_count": orphaned_count,
            "valid_documents_count": len(valid_document_ids)
        }
        
    except (VectorDBError, FileProcessingError) as e:
        logger.error(f"Error during cleanup: {e}")
        raise HTTPException(status_code=500, detail=f"ì •ë¦¬ ì‘ì—… ì˜¤ë¥˜: {e.message}")
    except Exception as e:
        logger.error(f"Unexpected error during cleanup: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="ì •ë¦¬ ì‘ì—… ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤")

@router.get("/storage/stats")
def get_storage_statistics():
    """
    ì €ì¥ì†Œ ì‚¬ìš©ëŸ‰ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        # íŒŒì¼ ì‹œìŠ¤í…œ í†µê³„
        file_stats = DocumentFileManager.get_storage_stats()
        
        # ë²¡í„° DB í†µê³„
        documents = get_all_documents()
        total_chunks = sum(doc.get("chunk_count", 0) for doc in documents)
        
        return {
            "file_storage": file_stats,
            "vector_db": {
                "total_documents": len(documents),
                "total_chunks": total_chunks
            }
        }
        
    except (VectorDBError, FileProcessingError) as e:
        logger.error(f"Error getting storage stats: {e}")
        raise HTTPException(status_code=500, detail=f"í†µê³„ ì¡°íšŒ ì˜¤ë¥˜: {e.message}")
    except Exception as e:
        logger.error(f"Unexpected error getting storage stats: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="í†µê³„ ì¡°íšŒ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤")

# ===== í™˜ì˜ë©”ì‹œì§€ API ì—”ë“œí¬ì¸íŠ¸ë“¤ =====

@router.get("/welcome-messages/random")
async def get_random_welcome_message():
    """
    ëœë¤ í™˜ì˜ë©”ì‹œì§€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        from app.services.welcome_message_service import get_random_welcome_message
        
        message = get_random_welcome_message()
        return {
            "message": message,
            "timestamp": __import__('datetime').datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error getting random welcome message: {e}")
        # ê¸°ë³¸ ë©”ì‹œì§€ ë°˜í™˜
        return {
            "message": "ğŸ“š ì•ˆë…•í•˜ì„¸ìš”! ì—…ë¡œë“œëœ ë¬¸ì„œë“¤ì— ëŒ€í•´ ê¶ê¸ˆí•œ ê²ƒì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ì„¸ìš”.",
            "timestamp": __import__('datetime').datetime.now().isoformat()
        }

@router.post("/welcome-messages/generate")
async def generate_welcome_messages(count: int = 5):
    """
    ìƒˆë¡œìš´ í™˜ì˜ë©”ì‹œì§€ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    try:
        from app.services.welcome_message_service import generate_welcome_messages
        
        if count < 1 or count > 10:
            raise HTTPException(status_code=400, detail="ìƒì„±í•  ë©”ì‹œì§€ ê°œìˆ˜ëŠ” 1-10ê°œ ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤")
        
        generated_count = generate_welcome_messages(count)
        
        return {
            "requested_count": count,
            "generated_count": generated_count,
            "message": f"í™˜ì˜ë©”ì‹œì§€ {generated_count}ê°œê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "timestamp": __import__('datetime').datetime.now().isoformat()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error generating welcome messages: {e}")
        raise HTTPException(status_code=500, detail=f"í™˜ì˜ë©”ì‹œì§€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@router.get("/welcome-messages")
async def get_all_welcome_messages():
    """
    ëª¨ë“  í™˜ì˜ë©”ì‹œì§€ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        from app.services.welcome_message_service import welcome_service
        
        messages = welcome_service.get_all_messages()
        doc_summary = welcome_service.get_document_summary()
        
        return {
            "messages": messages,
            "total_count": len(messages),
            "document_summary": doc_summary,
            "timestamp": __import__('datetime').datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error getting welcome messages: {e}")
        raise HTTPException(status_code=500, detail="í™˜ì˜ë©”ì‹œì§€ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤")

@router.delete("/welcome-messages/{message_id}")
async def delete_welcome_message(message_id: int):
    """
    íŠ¹ì • í™˜ì˜ë©”ì‹œì§€ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.
    """
    try:
        from app.services.welcome_message_service import welcome_service
        
        success = welcome_service.delete_message(message_id)
        
        if success:
            return {
                "message": f"í™˜ì˜ë©”ì‹œì§€ ID {message_id}ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.",
                "timestamp": __import__('datetime').datetime.now().isoformat()
            }
        else:
            raise HTTPException(status_code=404, detail=f"í™˜ì˜ë©”ì‹œì§€ ID {message_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error deleting welcome message {message_id}: {e}")
        raise HTTPException(status_code=500, detail="í™˜ì˜ë©”ì‹œì§€ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤")

@router.get("/welcome-messages/stats")
async def get_welcome_message_stats():
    """
    í™˜ì˜ë©”ì‹œì§€ í†µê³„ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        from app.services.welcome_message_service import get_welcome_message_stats
        
        stats = get_welcome_message_stats()
        return stats
        
    except Exception as e:
        logger.error(f"Error getting welcome message stats: {e}")
        raise HTTPException(status_code=500, detail="í™˜ì˜ë©”ì‹œì§€ í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤")

