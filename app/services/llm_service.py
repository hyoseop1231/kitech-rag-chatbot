import requests
import json
from typing import List, Dict, Any, Optional, Generator
from app.config import settings
from app.utils.logging_config import get_logger
from app.utils.exceptions import LLMError

logger = get_logger(__name__)

def construct_multimodal_rag_prompt(
    user_query: str,
    context_chunks: List[str],
    image_descriptions: List[str],
    table_contents: List[str],
    lang: str = "ko",
    conversation_history: List[Dict[str, str]] = None
) -> str:
    """
    Construct a multimodal RAG prompt combining text, images, and tables.
    
    Args:
        user_query: User's query
        context_chunks: List of relevant text chunks
        image_descriptions: List of image descriptions
        table_contents: List of table contents
        lang: Language code (default: "ko")
    
    Returns:
        str: Constructed prompt for LLM
    """
    if lang == "ko":
        prompt_parts = []
        
        # ëŒ€í™” ížˆìŠ¤í† ë¦¬ ì¶”ê°€ (ë§¥ë½ ì œê³µ)
        if conversation_history and len(conversation_history) > 0:
            logger.info(f"Adding {len(conversation_history)} conversation history messages to prompt")
            prompt_parts.extend([
                "=== ì´ì „ ëŒ€í™” ë‚´ìš© ===",
                "ë‹¤ìŒì€ ì´ì „ì— ë‚˜ëˆˆ ëŒ€í™”ìž…ë‹ˆë‹¤. ì´ ë§¥ë½ì„ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”:",
                ""
            ])
            
            # ìµœê·¼ ëŒ€í™”ë“¤ë§Œ í¬í•¨ (ìµœëŒ€ 3ìŒ)
            recent_history = conversation_history[-6:] if len(conversation_history) > 6 else conversation_history
            for msg in recent_history:
                role = "ì‚¬ìš©ìž" if msg.get("role") == "user" else "ì–´ì‹œìŠ¤í„´íŠ¸"
                content = msg.get("content", "").strip()
                if content:
                    prompt_parts.append(f"{role}: {content}")
            
            prompt_parts.extend(["", "=== í˜„ìž¬ ì§ˆë¬¸ ê´€ë ¨ ì •ë³´ ===", ""])
        
        prompt_parts.extend([
            "ë‹¤ìŒì€ ì£¼ì¡° ê¸°ìˆ  ë¬¸ì„œì—ì„œ ì¶”ì¶œí•œ ê´€ë ¨ ì •ë³´ìž…ë‹ˆë‹¤:",
            "",
            "=== í…ìŠ¤íŠ¸ ì •ë³´ ===",
        ])
        
        for i, chunk in enumerate(context_chunks, 1):
            prompt_parts.append(f"í…ìŠ¤íŠ¸ {i}: {chunk}")
            prompt_parts.append("")
        
        if image_descriptions:
            prompt_parts.append("=== ì´ë¯¸ì§€ ì •ë³´ ===")
            for i, desc in enumerate(image_descriptions, 1):
                if desc.strip():
                    prompt_parts.append(f"ì´ë¯¸ì§€ {i}: {desc}")
            prompt_parts.append("")
        
        if table_contents:
            prompt_parts.append("=== í‘œ ì •ë³´ ===")
            for i, table in enumerate(table_contents, 1):
                if table.strip():
                    prompt_parts.append(f"í‘œ {i}: {table}")
            prompt_parts.append("")
        
        prompt_parts.extend([
            "ìœ„ì˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”:",
            f"ì§ˆë¬¸: {user_query}",
            "",
            "ë‹µë³€ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ì¤€ìˆ˜í•´ì£¼ì„¸ìš”:",
            "1. ì´ì „ ëŒ€í™” ë‚´ìš©ì´ ìžˆë‹¤ë©´ ë°˜ë“œì‹œ ê·¸ ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”",
            "2. ì‚¬ìš©ìžê°€ 'ê·¸ê²ƒ', 'ê·¸ê±°', 'ì•žì—ì„œ ë§í•œ', 'ìœ„ì—ì„œ ì–¸ê¸‰í•œ' ë“±ì˜ í‘œí˜„ì„ ì‚¬ìš©í•˜ë©´ ì´ì „ ëŒ€í™”ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”",
            "3. ì œê³µëœ ì •ë³´ë§Œì„ ì‚¬ìš©í•˜ì—¬ ì •í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”", 
            "4. ê´€ë ¨ ì´ë¯¸ì§€ë‚˜ í‘œê°€ ìžˆë‹¤ë©´ í•´ë‹¹ ë‚´ìš©ì„ ì°¸ì¡°í•˜ì—¬ ì„¤ëª…í•˜ì„¸ìš”",
            "5. ì •ë³´ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ì—†ëŠ” ê²½ìš° ì†”ì§ížˆ ë§ì”€í•´ì£¼ì„¸ìš”",
            "",
            "âš¡ **ì¤‘ìš”: ë‹µë³€ í˜•ì‹ ë° ì‹œê°í™” ìš”êµ¬ì‚¬í•­**",
            "6. **êµ¬ì¡°í™”ëœ ë§ˆí¬ë‹¤ìš´ í˜•ì‹**ìœ¼ë¡œ ë‹µë³€í•˜ë˜, ë‹¤ìŒ ì‹œê°í™” ìš”ì†Œë“¤ì„ ì ê·¹ í™œìš©í•˜ì„¸ìš”:",
            "",
            "   ðŸ“Š **í‘œ í˜•ì‹ ë°ì´í„°**: ë¹„êµ, ë¶„ë¥˜, ìˆ˜ì¹˜ ë°ì´í„°ëŠ” ë°˜ë“œì‹œ í‘œë¡œ ì •ë¦¬",
            "   ```",
            "   | í•­ëª© | ê°’ | ë‹¨ìœ„ | ë¹„ê³  |",
            "   |------|----|----- |----- |",
            "   | ì˜ˆì‹œ | 100 | â„ƒ   | ê¶Œìž¥ê°’ |",
            "   ```",
            "",
            "   ðŸŽ¯ **ê°•ì¡° ë° ìƒ‰ê¹” êµ¬ì¡°í™”**: ì¤‘ìš” ì •ë³´ëŠ” ë‹¤ì–‘í•œ ë°©ì‹ìœ¼ë¡œ ê°•ì¡°",
            "   - **ðŸ”´ ìœ„í—˜/ì£¼ì˜ì‚¬í•­**: ë¹¨ê°„ìƒ‰ ê°•ì¡°ë¡œ í‘œì‹œ",
            "   - **ðŸŸ¡ ê¶Œìž¥ì‚¬í•­**: ë…¸ëž€ìƒ‰ ê°•ì¡°ë¡œ í‘œì‹œ", 
            "   - **ðŸŸ¢ ì •ìƒ/ì–‘í˜¸**: ì´ˆë¡ìƒ‰ ê°•ì¡°ë¡œ í‘œì‹œ",
            "   - **ðŸ”µ ì •ë³´/ì°¸ê³ **: íŒŒëž€ìƒ‰ ê°•ì¡°ë¡œ í‘œì‹œ",
            "",
            "   ðŸ“‹ **ë‹¨ê³„ë³„ í”„ë¡œì„¸ìŠ¤**: ë³µìž¡í•œ ê³¼ì •ì€ ë²ˆí˜¸ë‚˜ ì²´í¬ë¦¬ìŠ¤íŠ¸ë¡œ",
            "   ```",
            "   ### ðŸ“ ìž‘ì—… ìˆœì„œ",
            "   1. âœ… 1ë‹¨ê³„: ì¤€ë¹„ìž‘ì—…",
            "   2. âš™ï¸ 2ë‹¨ê³„: ì‹¤í–‰ê³¼ì •", 
            "   3. ðŸ” 3ë‹¨ê³„: ê²€ì¦ë‹¨ê³„",
            "   ```",
            "",
            "   ðŸ“ˆ **ê°œë…ë„/ë‹¤ì´ì–´ê·¸ëž¨**: ê´€ê³„ë‚˜ êµ¬ì¡° ì„¤ëª… ì‹œ ASCII ì•„íŠ¸ë‚˜ ë„ì‹í™”",
            "   ```",
            "   ì›ë£Œ â†’ [ìš©í•´] â†’ [ì •ì œ] â†’ [ì£¼ìž…] â†’ [ì‘ê³ ] â†’ ì™„ì„±í’ˆ",
            "     â†“      â†“       â†“       â†“       â†“",
            "   ì˜¨ë„ì²´í¬  ì„±ë¶„ì¡°ì •  ì†ë„ì œì–´  ì••ë ¥ê´€ë¦¬  í’ˆì§ˆê²€ì‚¬",
            "   ```",
            "",
            "   ðŸ’¡ **ë°•ìŠ¤ í˜•íƒœ ì •ë³´**: íŒ, ì£¼ì˜ì‚¬í•­, ìš”ì•½ ë“±ì€ ë°•ìŠ¤ë¡œ êµ¬ë¶„",
            "   ```",
            "   > ðŸ’¡ **ì „ë¬¸ê°€ íŒ**",
            "   > ì˜¨ë„ ê´€ë¦¬ê°€ í’ˆì§ˆì˜ 80%ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.",
            "   ```",
            "",
            "   ðŸ“Š **ìˆ˜ì¹˜ ë°ì´í„° ì‹œê°í™”**: ë²”ìœ„, ë¹„ìœ¨, ì„±ëŠ¥ ì§€í‘œ ë“±",
            "   ```",
            "   ì ì • ì˜¨ë„: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ 80% (1200-1250â„ƒ)",
            "   ì••ë ¥ ë²”ìœ„: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ 60% (2-3 bar)",
            "   ```",
            "",
            "7. **ìƒì„¸í•˜ê³  êµ¬ì²´ì ì¸ ë‚´ìš©**: ë‹¨ìˆœ ì„¤ëª…ë³´ë‹¤ëŠ” ì‹¤ë¬´ì— ë„ì›€ë˜ëŠ” êµ¬ì²´ì  ì •ë³´ ì œê³µ",
            "   - ìˆ˜ì¹˜, ë²”ìœ„, ê¸°ì¤€ê°’ ëª…ì‹œ",
            "   - ì›ì¸ê³¼ ê²°ê³¼ì˜ ê´€ê³„ ì„¤ëª…",
            "   - ì‹¤ì œ ì ìš© ë°©ë²•ê³¼ ì£¼ì˜ì‚¬í•­",
            "   - í’ˆì§ˆ ê¸°ì¤€ ë° í‰ê°€ ë°©ë²•",
            "",
            "8. **ì „ë¬¸ìš©ì–´ ì„¤ëª…**: ê¸°ìˆ ìš©ì–´ ì‚¬ìš© ì‹œ ê´„í˜¸ ì•ˆì— ì‰¬ìš´ ì„¤ëª… ì¶”ê°€",
            "   ì˜ˆ: íƒ•êµ¬ì‹œìŠ¤í…œ(ìš©ìœµê¸ˆì†ì´ íë¥´ëŠ” í†µë¡œ) ì„¤ê³„",
            "",
            "9. **ì‹¤ë¬´ ì¤‘ì‹¬ ì ‘ê·¼**: ì´ë¡ ë³´ë‹¤ëŠ” ì‹¤ì œ í˜„ìž¥ì—ì„œ í™œìš©í•  ìˆ˜ ìžˆëŠ” ì •ë³´ ìš°ì„ ",
            "",
            "ë‹µë³€:"
        ])
    else:
        prompt_parts = []
        
        # Add conversation history (for context)
        if conversation_history and len(conversation_history) > 0:
            prompt_parts.extend([
                "=== Previous Conversation ===",
                "Here is our previous conversation. Please refer to this context for your answer:",
                ""
            ])
            
            # Include only recent conversations (max 3 pairs)
            recent_history = conversation_history[-6:] if len(conversation_history) > 6 else conversation_history
            for msg in recent_history:
                role = "User" if msg.get("role") == "user" else "Assistant"
                content = msg.get("content", "").strip()
                if content:
                    prompt_parts.append(f"{role}: {content}")
            
            prompt_parts.extend(["", "=== Current Question Information ===", ""])
        
        prompt_parts.extend([
            "The following is relevant information extracted from foundry technology documents:",
            "",
            "=== Text Information ===",
        ])
        
        for i, chunk in enumerate(context_chunks, 1):
            prompt_parts.append(f"Text {i}: {chunk}")
            prompt_parts.append("")
        
        if image_descriptions:
            prompt_parts.append("=== Image Information ===")
            for i, desc in enumerate(image_descriptions, 1):
                if desc.strip():
                    prompt_parts.append(f"Image {i}: {desc}")
            prompt_parts.append("")
        
        if table_contents:
            prompt_parts.append("=== Table Information ===")
            for i, table in enumerate(table_contents, 1):
                if table.strip():
                    prompt_parts.append(f"Table {i}: {table}")
            prompt_parts.append("")
        
        prompt_parts.extend([
            "Based on the above information, please answer the following question:",
            f"Question: {user_query}",
            "",
            "Please follow these guidelines when answering:",
            "1. If there is previous conversation history, always consider that context in your response",
            "2. When users use words like 'it', 'that', 'mentioned above', 'previously discussed', refer to the conversation history",
            "3. Use only the provided information to give accurate and specific answers",
            "4. If there are related images or tables, refer to them in your explanation", 
            "5. Be honest if information is insufficient or unavailable",
            "",
            "âš¡ **Important: Answer Format & Visualization Requirements**",
            "6. **Structured Markdown Format** with extensive use of the following visualization elements:",
            "",
            "   ðŸ“Š **Tabular Data**: Always organize comparisons, classifications, and numerical data in tables",
            "   ```",
            "   | Item | Value | Unit | Notes |",
            "   |------|-------|------|-------|",
            "   | Example | 100 | â„ƒ | Recommended |",
            "   ```",
            "",
            "   ðŸŽ¯ **Emphasis & Color Coding**: Highlight important information with various methods",
            "   - **ðŸ”´ Danger/Warnings**: Mark with red emphasis",
            "   - **ðŸŸ¡ Recommendations**: Mark with yellow emphasis", 
            "   - **ðŸŸ¢ Normal/Good**: Mark with green emphasis",
            "   - **ðŸ”µ Information/Reference**: Mark with blue emphasis",
            "",
            "   ðŸ“‹ **Step-by-Step Processes**: Present complex procedures as numbered lists or checklists",
            "   ```",
            "   ### ðŸ“ Work Sequence",
            "   1. âœ… Step 1: Preparation",
            "   2. âš™ï¸ Step 2: Execution", 
            "   3. ðŸ” Step 3: Verification",
            "   ```",
            "",
            "   ðŸ“ˆ **Concept Diagrams**: Use ASCII art or flowcharts for relationships and structures",
            "   ```",
            "   Raw Material â†’ [Melting] â†’ [Refining] â†’ [Pouring] â†’ [Solidification] â†’ Final Product",
            "        â†“           â†“          â†“          â†“             â†“",
            "   Temp Check   Composition  Speed Control  Pressure Mgmt  Quality Test",
            "   ```",
            "",
            "   ðŸ’¡ **Information Boxes**: Use boxes for tips, warnings, and summaries",
            "   ```",
            "   > ðŸ’¡ **Expert Tip**",
            "   > Temperature control determines 80% of quality.",
            "   ```",
            "",
            "   ðŸ“Š **Numerical Data Visualization**: Show ranges, ratios, performance indicators",
            "   ```",
            "   Optimal Temp: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ 80% (1200-1250â„ƒ)",
            "   Pressure Range: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ 60% (2-3 bar)",
            "   ```",
            "",
            "7. **Detailed and Specific Content**: Provide practical information rather than simple explanations",
            "   - Include specific numbers, ranges, and standard values",
            "   - Explain cause-and-effect relationships",
            "   - Provide practical application methods and precautions",
            "   - Include quality standards and evaluation methods",
            "",
            "8. **Technical Term Explanations**: Add simple explanations in parentheses for technical terms",
            "   Example: Gating system (channels through which molten metal flows) design",
            "",
            "9. **Practical Focus**: Prioritize information that can be used in actual field work over theory",
            "",
            "Answer:"
        ])
    
    return "\n".join(prompt_parts)

def get_llm_response_stream(prompt: str, model_name: str = None, options: Dict = None) -> Generator[str, None, None]:
    """
    Sends a prompt to the Ollama LLM and gets a streaming response.
    """
    if not model_name:
        model_name = settings.OLLAMA_DEFAULT_MODEL

    payload = {
        "model": model_name,
        "prompt": prompt,
        "stream": True,
    }
    if options:
        payload["options"] = options

    logger.info(f"Sending streaming prompt to Ollama model '{model_name}'. Prompt length: {len(prompt)} chars")
    
    try:
        response = requests.post(
            settings.OLLAMA_API_URL, 
            json=payload, 
            stream=True,
            timeout=settings.OLLAMA_TIMEOUT
        )

        if response.status_code != 200:
            error_msg = f"Ollama API request failed with status {response.status_code}: {response.text}"
            logger.error(error_msg)
            raise LLMError(error_msg)

        # Stream response - yield chunks as they come
        logger.debug("Streaming response from Ollama...")
        for line in response.iter_lines():
            if line:
                try:
                    json_response = json.loads(line.decode('utf-8'))
                    if 'response' in json_response:
                        yield json_response['response']
                    if json_response.get('done', False):
                        break
                except json.JSONDecodeError as e:
                    logger.warning(f"Failed to parse JSON response line: {line}, error: {e}")
                    continue

    except requests.exceptions.Timeout:
        error_msg = f"Ollama request timed out after {settings.OLLAMA_TIMEOUT} seconds"
        logger.error(error_msg)
        raise LLMError(error_msg)
    except requests.exceptions.ConnectionError:
        error_msg = "Failed to connect to Ollama. Please ensure Ollama is running."
        logger.error(error_msg)
        raise LLMError(error_msg)
    except Exception as e:
        error_msg = f"Unexpected error during Ollama request: {str(e)}"
        logger.error(error_msg)
        raise LLMError(error_msg)

def get_llm_response(prompt: str, model_name: str = None, options: Dict = None, stream: bool = False) -> str | Generator[str, None, None]:
    """
    Sends a prompt to the Ollama LLM and gets a response.

    Args:
        prompt (str): The prompt to send to the LLM.
        model_name (str, optional): The name of the Ollama model to use.
                                    Defaults to DEFAULT_MODEL if not provided.
        options (Dict, optional): Additional Ollama model parameters (e.g., temperature).
                                  Refer to Ollama API documentation for available options.
        stream (bool, optional): Whether to stream the response. Defaults to False.
                                 If True, the function will yield chunks of the response.

    Returns:
        str: The LLM's response text if stream is False.
        Generator[str, None, None]: A generator that yields chunks of the LLM's response if stream is True.
    """
    if stream:
        return get_llm_response_stream(prompt, model_name, options)
    
    # Non-streaming response
    if not model_name:
        model_name = settings.OLLAMA_DEFAULT_MODEL

    payload = {
        "model": model_name,
        "prompt": prompt,
        "stream": False,
    }
    if options:
        payload["options"] = options

    logger.info(f"Sending prompt to Ollama model '{model_name}'. Prompt length: {len(prompt)} chars")
    logger.debug(f"Prompt preview: {prompt[:200]}...")

    try:
        response = requests.post(
            settings.OLLAMA_API_URL, 
            json=payload, 
            stream=True,
            timeout=settings.OLLAMA_TIMEOUT
        )

        if response.status_code != 200:
            error_msg = f"Ollama API request failed with status {response.status_code}: {response.text}"
            logger.error(error_msg)
            raise LLMError(error_msg)

        # Non-streaming response - collect all chunks and return as string
        full_response = ""
        for line in response.iter_lines():
            if line:
                try:
                    json_response = json.loads(line.decode('utf-8'))
                    if 'response' in json_response:
                        full_response += json_response['response']
                    if json_response.get('done', False):
                        break
                except json.JSONDecodeError as e:
                    logger.warning(f"Failed to parse JSON response line: {line}, error: {e}")
                    continue
        
        logger.info(f"Received complete response from Ollama. Length: {len(full_response)} chars")
        return full_response.strip()

    except requests.exceptions.Timeout:
        error_msg = f"Ollama request timed out after {settings.OLLAMA_TIMEOUT} seconds"
        logger.error(error_msg)
        raise LLMError(error_msg)
    except requests.exceptions.ConnectionError:
        error_msg = "Failed to connect to Ollama. Please ensure Ollama is running."
        logger.error(error_msg)
        raise LLMError(error_msg)
    except Exception as e:
        error_msg = f"Unexpected error during Ollama request: {str(e)}"
        logger.error(error_msg)
        raise LLMError(error_msg)

def get_llm_response_async(prompt: str, model_name: str = None, options: Dict = None) -> str:
    """
    Asynchronous version of get_llm_response for non-streaming requests.
    
    Args:
        prompt (str): The prompt to send to the LLM.
        model_name (str, optional): The name of the Ollama model to use.
        options (Dict, optional): Additional Ollama model parameters.
    
    Returns:
        str: The LLM's response text.
    """
    return get_llm_response(prompt, model_name, options, stream=False)