"""
Ollama ëª¨ë¸ ì •ë³´ ìºì‹± ë° ê´€ë¦¬ ì„œë¹„ìŠ¤
LLMìœ¼ë¡œë¶€í„° ì§ì ‘ ëª¨ë¸ ì •ë³´ë¥¼ ê°€ì ¸ì™€ì„œ ìºì‹±
"""
import json
import time
import requests
from typing import Dict, List, Optional, Any
from pathlib import Path
from app.config import settings
from app.utils.logging_config import get_logger
from app.services.llm_service import get_llm_response

logger = get_logger(__name__)

class ModelInfoCache:
    """ëª¨ë¸ ì •ë³´ ìºì‹± ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.cache_file = Path("app/data/model_info_cache.json")
        self.cache_file.parent.mkdir(exist_ok=True)
        self.cache_data = self._load_cache()
        self.cache_ttl = 24 * 60 * 60  # 24ì‹œê°„
        
    def _load_cache(self) -> Dict[str, Any]:
        """ìºì‹œ íŒŒì¼ ë¡œë“œ"""
        try:
            if self.cache_file.exists():
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            logger.error(f"Failed to load model info cache: {e}")
        
        return {
            "last_updated": 0,
            "model_count": 0,
            "models": {}
        }
    
    def _save_cache(self):
        """ìºì‹œ íŒŒì¼ ì €ì¥"""
        try:
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.cache_data, f, ensure_ascii=False, indent=2)
            logger.info(f"Model info cache saved: {len(self.cache_data['models'])} models")
        except Exception as e:
            logger.error(f"Failed to save model info cache: {e}")
    
    def _get_current_models(self) -> List[str]:
        """í˜„ì¬ Ollamaì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        base = settings.OLLAMA_API_URL
        urls = [
            base.replace('/api/generate', '/api/models'),
            base.replace('/api/generate', '/api/tags'),
        ]
        
        for url in urls:
            try:
                resp = requests.get(url, timeout=settings.OLLAMA_TIMEOUT)
                if resp.status_code == 200:
                    data = resp.json()
                    
                    # ì‘ë‹µ êµ¬ì¡° íŒŒì‹±
                    if isinstance(data, dict) and 'models' in data:
                        mlist = data.get('models', [])
                        if mlist and isinstance(mlist[0], dict) and 'name' in mlist[0]:
                            return [m.get('name', '') for m in mlist]
                        else:
                            return list(mlist)
                    elif isinstance(data, list):
                        return data
                    
            except Exception as e:
                logger.warning(f"Failed to fetch models from {url}: {e}")
                continue
        
        logger.error("Failed to fetch model list from Ollama")
        return []
    
    def _get_model_info_from_llm(self, model_name: str) -> Dict[str, str]:
        """LLMì—ê²Œ ëª¨ë¸ ì •ë³´ë¥¼ ì§ˆì˜ (ê°„ë‹¨í•˜ê³  ë¹ ë¥´ê²Œ)"""
        prompt = f"""Model: {model_name}

Please provide brief info in JSON format:
{{
    "display_name": "Clean display name",
    "provider": "Company (Meta/Google/Mistral/etc)",
    "size": "Model size (7B/13B/etc)",
    "type": "Main purpose (Chat/Code/Reasoning/etc)"
}}

Keep it very brief and factual. Use "Unknown" if unsure."""

        try:
            # ë” ì§§ì€ íƒ€ì„ì•„ì›ƒê³¼ ê°„ë‹¨í•œ ì˜µì…˜ìœ¼ë¡œ ë¹ ë¥´ê²Œ ì²˜ë¦¬
            response = get_llm_response(
                prompt, 
                model_name=model_name, 
                options={"num_predict": 200, "temperature": 0.1},  # ì§§ê³  ì¼ê´€ëœ ì‘ë‹µ
                stream=False
            )
            
            # JSON ì¶”ì¶œ ì‹œë„
            response = response.strip()
            if response.startswith('```'):
                lines = response.split('\n')
                json_lines = []
                in_json = False
                for line in lines:
                    if line.startswith('```') and 'json' in line:
                        in_json = True
                        continue
                    elif line.startswith('```'):
                        break
                    elif in_json:
                        json_lines.append(line)
                response = '\n'.join(json_lines)
            
            # JSON íŒŒì‹±
            info = json.loads(response)
            
            # ê°„ë‹¨í•œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            simplified_info = {
                "display_name": info.get("display_name", model_name),
                "provider": info.get("provider", "Unknown"),
                "model_size": info.get("size", "Unknown"),
                "specialization": info.get("type", "Unknown"),
                "description": f"{info.get('provider', 'Unknown')} {info.get('type', 'AI')} ëª¨ë¸",
                "strengths": info.get("type", "Unknown"),
                "best_for": info.get("type", "Unknown")
            }
            
            logger.info(f"Successfully retrieved info for model: {model_name}")
            return simplified_info
            
        except Exception as e:
            logger.warning(f"Failed to get model info for {model_name}: {e}")
        
        # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì •ë³´ ë°˜í™˜ (ë¹ ë¥´ê²Œ)
        return {
            "display_name": model_name.split(':')[0].title(),
            "provider": "Unknown",
            "model_size": "Unknown",
            "specialization": "AI Assistant", 
            "description": f"{model_name} AI ëª¨ë¸",
            "strengths": "General AI",
            "best_for": "General tasks"
        }
    
    def _needs_update(self, current_models: List[str]) -> bool:
        """ìºì‹œ ì—…ë°ì´íŠ¸ê°€ í•„ìš”í•œì§€ í™•ì¸"""
        # TTL í™•ì¸
        current_time = time.time()
        if current_time - self.cache_data.get("last_updated", 0) > self.cache_ttl:
            logger.info("Model cache TTL expired")
            return True
        
        # ëª¨ë¸ ìˆ˜ ë³€ê²½ í™•ì¸
        if len(current_models) != self.cache_data.get("model_count", 0):
            logger.info(f"Model count changed: {self.cache_data.get('model_count', 0)} -> {len(current_models)}")
            return True
        
        # ìƒˆë¡œìš´ ëª¨ë¸ ì¶”ê°€ í™•ì¸
        cached_models = set(self.cache_data.get("models", {}).keys())
        current_models_set = set(current_models)
        
        if not current_models_set.issubset(cached_models):
            new_models = current_models_set - cached_models
            logger.info(f"New models detected: {list(new_models)}")
            return True
        
        return False
    
    def get_model_info(self, force_refresh: bool = False) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ìºì‹± ì ìš©)"""
        current_models = self._get_current_models()
        
        if not current_models:
            logger.error("No models available from Ollama")
            return {"models": [], "summaries": [], "tips": []}
        
        # ì—…ë°ì´íŠ¸ í•„ìš”ì„± í™•ì¸
        if force_refresh or self._needs_update(current_models):
            logger.info("Updating model info cache...")
            self._update_cache(current_models)
        else:
            logger.info("Using cached model info")
        
        # ìºì‹œëœ ì •ë³´ ë°˜í™˜
        cached_models = self.cache_data.get("models", {})
        
        summaries = []
        for model_name in current_models:
            if model_name in cached_models:
                info = cached_models[model_name]
                summaries.append({
                    "name": model_name,
                    "display_name": info.get("display_name", model_name),
                    "provider": info.get("provider", "ì•Œ ìˆ˜ ì—†ìŒ"),
                    "params": f"{info.get('model_size', 'N/A')} | {info.get('specialization', 'N/A')}",
                    "key_points": info.get("description", "")
                })
        
        # ì‚¬ìš© íŒ ìƒì„±
        tips = [
            "ğŸ’¡ ì¶”ë¡ Â·ì½”ë”© ë¬¸ì œ í•´ê²° â†’ Qwen, DeepSeek ê³„ì—´ ëª¨ë¸ ì¶”ì²œ",
            "ğŸ‡°ğŸ‡· í•œêµ­ì–´ ì§ˆì˜ì‘ë‹µ â†’ í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ ë˜ëŠ” ë‹¤êµ­ì–´ ëª¨ë¸ ì‚¬ìš©",
            "ğŸ“Š RAGÂ·ê²€ìƒ‰ ì„ë² ë”© â†’ SFR-Embedding, BGE ê³„ì—´ ëª¨ë¸ ì¶”ì²œ",
            "âš¡ ë¦¬ì†ŒìŠ¤ ì œì•½ í™˜ê²½ â†’ 7B ì´í•˜ ê²½ëŸ‰ ëª¨ë¸ ì¶”ì²œ"
        ]
        
        return {
            "models": current_models,
            "summaries": summaries,
            "tips": tips
        }
    
    def _update_cache(self, current_models: List[str]):
        """ìºì‹œ ì—…ë°ì´íŠ¸"""
        updated_models = {}
        
        # ê¸°ì¡´ ìºì‹œëœ ëª¨ë¸ ì •ë³´ ìœ ì§€
        existing_models = self.cache_data.get("models", {})
        
        for model_name in current_models:
            if model_name in existing_models:
                # ê¸°ì¡´ ì •ë³´ ìœ ì§€
                updated_models[model_name] = existing_models[model_name]
                logger.debug(f"Keeping cached info for: {model_name}")
            else:
                # ìƒˆ ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                logger.info(f"Fetching info for new model: {model_name}")
                model_info = self._get_model_info_from_llm(model_name)
                updated_models[model_name] = model_info
                time.sleep(0.5)  # LLM ìš”ì²­ ê°„ ì§€ì—° (ë‹¨ì¶•)
        
        # ìºì‹œ ì—…ë°ì´íŠ¸
        self.cache_data = {
            "last_updated": time.time(),
            "model_count": len(current_models),
            "models": updated_models
        }
        
        self._save_cache()
        logger.info(f"Model cache updated with {len(updated_models)} models")
    
    def refresh_model_info(self, model_name: str = None):
        """íŠ¹ì • ëª¨ë¸ ë˜ëŠ” ì „ì²´ ëª¨ë¸ ì •ë³´ ê°•ì œ ìƒˆë¡œê³ ì¹¨"""
        if model_name:
            logger.info(f"Refreshing info for model: {model_name}")
            model_info = self._get_model_info_from_llm(model_name)
            
            if "models" not in self.cache_data:
                self.cache_data["models"] = {}
            
            self.cache_data["models"][model_name] = model_info
            self._save_cache()
        else:
            logger.info("Refreshing all model info")
            self.get_model_info(force_refresh=True)

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
model_info_cache = ModelInfoCache()

def get_cached_model_info(force_refresh: bool = False) -> Dict[str, Any]:
    """ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (í¸ì˜ í•¨ìˆ˜)"""
    return model_info_cache.get_model_info(force_refresh)

def refresh_model_cache(model_name: str = None):
    """ëª¨ë¸ ìºì‹œ ìƒˆë¡œê³ ì¹¨ (í¸ì˜ í•¨ìˆ˜)"""
    model_info_cache.refresh_model_info(model_name)