"""
Streaming service for real-time LLM response delivery
"""
from typing import Dict, Any, Generator, List
from app.services.llm_service import get_llm_response, construct_multimodal_rag_prompt
from app.config import settings
from app.utils.logging_config import get_logger

logger = get_logger(__name__)

def generate_consistent_references(multimodal_content: Dict[str, Any]) -> str:
    """
    Generate consistent reference format from multimodal content
    
    Args:
        multimodal_content: Retrieved content with text, images, tables
        
    Returns:
        str: Formatted reference section
    """
    references = []
    
    # Extract text sources
    text_data = multimodal_content.get("text", multimodal_content.get("text_chunks", []))
    text_sources = set()
    
    if isinstance(text_data, list) and text_data:
        for chunk in text_data:
            if isinstance(chunk, dict):
                source = chunk.get("metadata", {}).get("source_document_id", "")
                if source:
                    text_sources.add(source)
            
    for source in sorted(text_sources):
        references.append(f"ğŸ“„ {source}")
    
    # Extract table sources  
    tables = multimodal_content.get("tables", [])
    for i, table in enumerate(tables, 1):
        if isinstance(table, dict):
            metadata = table.get("metadata", {})
            source = metadata.get("source_document_id", "")
            page = metadata.get("page", "")
            if source:
                page_info = f" (í˜ì´ì§€ {page})" if page else ""
                references.append(f"ğŸ“Š í‘œ{i} - {source}{page_info}")
    
    # Extract image sources
    images = multimodal_content.get("images", [])
    for i, image in enumerate(images, 1):
        if isinstance(image, dict):
            metadata = image.get("metadata", {})
            source = metadata.get("source_document_id", "")
            page = metadata.get("page", "")
            if source:
                page_info = f" (í˜ì´ì§€ {page})" if page else ""
                references.append(f"ğŸ–¼ï¸ ì´ë¯¸ì§€{i} - {source}{page_info}")
    
    if references:
        return f"\n\n## ğŸ“š ì°¸ê³ ë¬¸í—Œ\n" + "\n".join(f"- {ref}" for ref in references)
    
    return ""

def process_multimodal_llm_chat_request_stream(
    user_query: str,
    multimodal_content: Dict[str, Any],
    model_name: str = None,
    lang: str = "ko",
    options: Dict = None,
    conversation_history: List[Dict[str, str]] = None
) -> Generator[str, None, None]:
    """
    Process multimodal chat request with streaming response.
    
    Args:
        user_query: User's question
        multimodal_content: Retrieved content (text, images, tables)
        model_name: LLM model to use
        lang: Language for response
        options: LLM options
        
    Yields:
        str: Streaming response chunks
    """
    try:
        # Extract content from multimodal data - handle both key formats for compatibility
        text_data = multimodal_content.get("text", multimodal_content.get("text_chunks", []))
        if isinstance(text_data, list) and text_data and isinstance(text_data[0], dict):
            # Format: [{"text": "...", "metadata": {...}}, ...]
            context_chunks = [chunk.get("text", "") for chunk in text_data]
        elif isinstance(text_data, list):
            # Format: ["text1", "text2", ...]
            context_chunks = text_data
        else:
            context_chunks = []
        
        images = multimodal_content.get("images", [])
        image_descriptions = [img.get("description", "") for img in images]
        
        tables = multimodal_content.get("tables", [])
        table_contents = [table.get("content", "") for table in tables]
        
        # Construct the multimodal prompt
        prompt = construct_multimodal_rag_prompt(
            user_query,
            context_chunks,
            image_descriptions,
            table_contents,
            lang,
            conversation_history
        )
        
        logger.info(f"Starting streaming response for query: {user_query[:50]}...")
        
        # Get streaming response from LLM
        stream_generator = get_llm_response(
            prompt=prompt,
            model_name=model_name,
            options=options,
            stream=True  # Enable streaming
        )
        
        # Track the full response as we stream it
        full_response_chunks = []
        response_complete = False
        
        for chunk in stream_generator:
            if chunk:  # Only yield non-empty chunks
                yield chunk
                full_response_chunks.append(chunk)
                response_complete = True
        
        # Add consistent references at the end if response was generated AND meaningful
        if response_complete:
            # Reconstruct full response for analysis
            full_response_text = "".join(full_response_chunks)
            
            # Use the same meaningful response logic as multimodal_llm_service.py
            meaningful_response = (
                len(full_response_text) > 500 and  # Response is substantial
                not any(phrase in full_response_text.lower() for phrase in [
                    "ì •ë³´ê°€ ë¶€ì¡±", "í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†", "ë‚´ìš©ì„ íŒŒì•…í•˜ê¸° ì–´ë µ",
                    "êµ¬ì²´ì ì¸ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ì—†", "ì •ë³´ë¡œëŠ”", "ì–¸ê¸‰ë˜ì–´ ìˆì§€ ì•Š",
                    "í˜„ì¬ ì œê³µëœ ì •ë³´ë§Œìœ¼ë¡œëŠ”", "ì¶”ê°€ì ì¸ ì •ë³´ê°€ í•„ìš”", "íŒŒì•…í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤",
                    "í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤", "ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ì—†",
                    "ê´€ë ¨ì´ ì—†ìŠµë‹ˆë‹¤", "ì „í˜€ í¬í•¨ë˜ì–´ ìˆì§€ ì•Š", "ì–´ë– í•œì§€ ë‹µë³€í•  ìˆ˜ ì—†",
                    "ë‚ ì”¨ ì •ë³´ì™€ëŠ” ê´€ë ¨ì´ ì—†", "ë¬¸ì„œ ì •ë³´ì—ëŠ”", "í¬í•¨ë˜ì–´ ìˆì§€ ì•Šì•„", 
                    "ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "ë¬¸ì„œì˜ ë²”ìœ„ ë°–", "ë²”ìœ„ ë°–ì˜ ë‚´ìš©", "í¬í•¨ë˜ì–´ ìˆì§€ ì•Š",
                    "ë‚ ì”¨ì— ëŒ€í•œ ë‹µë³€", "ë‚ ì”¨ì— ëŒ€í•œ ë‚´ìš©ì´ í¬í•¨", "ë‚ ì”¨ ì •ë³´ëŠ”"
                ])
            )
            
            # Only add references if we have meaningful content
            if meaningful_response:
                references = generate_consistent_references(multimodal_content)
                if references:
                    yield references
                
        logger.info("Streaming response completed")
        
    except Exception as e:
        logger.error(f"Error in streaming response: {e}")
        yield f"\n\n[ì˜¤ë¥˜: {str(e)}]"